{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and exploration of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* Exploration of the Datasets - Augmented Milestone 2 \n",
    "    * Pre-processing : checking for NaN values\n",
    "        * Beer datasets\n",
    "        * User datasets\n",
    "        * Brewery datasets\n",
    "    * Exploration of the datasets\n",
    "        * Textual reviews\n",
    "* Analysis for Data Story - Milestone 3\n",
    "    * Pre-processing for bias correction\n",
    "    * Bias correction\n",
    "    * Beer characteristics\n",
    "    * SAT dataset matches with BeerAdvocate and RateBeer\n",
    "    * Querying the dataset for the beers prefered by each country\n",
    "    \n",
    "* Graphs and Plots for Data Story\n",
    "    * t-SNE of SAT beers and countries preferences\n",
    "    * SAT beer ranking\n",
    "    * wordclouds\n",
    "    * Interactive World Map\n",
    "    \n",
    "* Addendum : Our helper modules exposed for easy reading and code-checking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:40.122312Z",
     "start_time": "2022-12-22T16:20:40.106058Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import of all the necessary libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import plotly.tools \n",
    "import plotly.graph_objs as go\n",
    "from  plotly.offline import plot\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "from plotly.subplots import make_subplots \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,silhouette_score\n",
    "from statsmodels.stats import diagnostic\n",
    "import openai\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#We created a certain number of modules to encapsulate functions we created for this project\n",
    "\n",
    "from functions import read_data\n",
    "from functions import NLP_utils\n",
    "from functions import SAT_helpers\n",
    "from functions import plot_helpers\n",
    "from functions.happyness_helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:42.710883Z",
     "start_time": "2022-12-22T16:20:41.547625Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading the datasets: \n",
    "\n",
    "DATASET_BEERADVOCATE = 'DATA/BeerAdvocate/'\n",
    "DATASET_RATEBEER = 'DATA/RateBeer/'\n",
    "DATASET_MATCHEDBEER = 'DATA/matched_beer_data/'\n",
    "\n",
    "\n",
    "#Beer advocate dataset\n",
    "df_adv_beer = pd.read_csv(DATASET_BEERADVOCATE + 'beers.csv')\n",
    "df_adv_breweries = pd.read_csv(DATASET_BEERADVOCATE + 'breweries.csv')\n",
    "df_adv_users = pd.read_csv(DATASET_BEERADVOCATE + 'users.csv')\n",
    "\n",
    "#Ratebeer dataset\n",
    "df_rb_beer = pd.read_csv(DATASET_RATEBEER + 'beers.csv')\n",
    "df_rb_breweries = pd.read_csv(DATASET_RATEBEER + 'breweries.csv')\n",
    "df_rb_users = pd.read_csv(DATASET_RATEBEER + 'users.csv')\n",
    "\n",
    "#Matched dataset - We do not use it in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "\n",
    "## Pre-processing: checking for NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beer_processing'></a>\n",
    "\n",
    "### Beer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:44.072043Z",
     "start_time": "2022-12-22T16:20:44.034122Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's have a look and check the shape of the datasets: \n",
    "print(\"BeerAdvocate beers:\")\n",
    "display(df_adv_beer.head(2))\n",
    "print(\"Beer advocate beer dataset has {} rows and {} colomns\".format(df_adv_beer.shape[0],df_adv_beer.shape[1]))\n",
    "print(\"RateBeer beers:\")\n",
    "display(df_rb_beer.head(2))\n",
    "print(\"RateBeer beer dataset has {} rows and {} colomns\".format(df_rb_beer.shape[0],df_rb_beer.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:45.251187Z",
     "start_time": "2022-12-22T16:20:44.787178Z"
    }
   },
   "outputs": [],
   "source": [
    "#We are looking at the proportion of NaNs in each column of the beer datasets.\n",
    "##Creation of 3 dataframes, one for each dataset having 3 columns\n",
    "list = ['Total','NaN_proportion','NaN_number']\n",
    "dfnan_adv = pd.DataFrame(columns = list)\n",
    "dfnan_rate = pd.DataFrame(columns = list)\n",
    "\n",
    "\n",
    "dfnan_adv['NaN_proportion'] = (df_adv_beer.isna().sum()/df_adv_beer.shape[0]) #proportion of NaN values in each column \n",
    "dfnan_adv['NaN_number'] = (df_adv_beer.isna().sum()) #total number of NaN values in each column \n",
    "dfnan_adv.loc[:,'Total'] = df_adv_beer.shape[0] #Number of rows in the dataset\n",
    "\n",
    "dfnan_rate['NaN_proportion'] = (df_rb_beer.isna().sum()/df_rb_beer.shape[0])\n",
    "dfnan_rate['NaN_number'] = (df_rb_beer.isna().sum())\n",
    "dfnan_rate.loc[:,'Total'] = df_rb_beer.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:45.645209Z",
     "start_time": "2022-12-22T16:20:45.252517Z"
    }
   },
   "outputs": [],
   "source": [
    "#We will look at the distribution of NaNs in each dataset. Here we plot the distribution of NaNs for beers.csv for RateBeer a d BeerAdvocate\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(11,6))\n",
    "\n",
    "ax1 = plt.subplot(221)\n",
    "ax2 = plt.subplot(222)\n",
    "\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"Total\", y = dfnan_adv.index, data=dfnan_adv, label=\"Valid values\", color=\"b\", ax=ax1)\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='NaN_proportion', y = dfnan_adv.index, data=dfnan_adv, label=\"NaN Values\", color=\"b\", ax=ax1)\n",
    "ax1.legend(ncol=1, loc=\"upper right\", frameon=True)\n",
    "ax1.set(xlim=(0, 1), ylabel=\"\", xlabel=\"NaN values BeerAdvocate\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"Total\", y = dfnan_rate.index, data=dfnan_rate, label=\"Valid Values\", color=\"b\",ax=ax2)\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='NaN_proportion',  y = dfnan_rate.index, data=dfnan_rate, label=\"NaN\", color=\"b\", ax=ax2)\n",
    "ax2.legend(ncol=1, loc=\"upper right\", frameon=True)\n",
    "ax2.set(xlim=(0, 1), ylabel=\"\", xlabel=\"NaN values RateBeer\") \n",
    "\n",
    "plt.suptitle('Missing values proportion per columns per beer dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that several columns in the beer datasets display an important NaN content. For instance, there is around 90% NaN values of avg_matched_valid_ratings in both datasets. For each dataset, 4 columns have high NaN content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:45.752062Z",
     "start_time": "2022-12-22T16:20:45.738944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking missing values in  breweries from all dataset \n",
    "# we see that there are no missing values\n",
    "\n",
    "\n",
    "print('Test of the presences of NaN in the entire breweries datasets from AdvocatedBeer and RateBeer:', df_adv_breweries.isna().values.any(),\n",
    "      'and ', df_rb_breweries.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:46.170016Z",
     "start_time": "2022-12-22T16:20:46.104861Z"
    }
   },
   "outputs": [],
   "source": [
    "# analyze missing values in the AdvocateBeer breweries dataset and see that there are less than 30% of missing values \n",
    "df_adv_users.isna().sum()/df_adv_users.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:46.422413Z",
     "start_time": "2022-12-22T16:20:46.389802Z"
    }
   },
   "outputs": [],
   "source": [
    "# analyze missing values in the RateBeer breweries dataset and see that there are less thqn 30% of missing values \n",
    "df_rb_users.isna().sum()/df_rb_users.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that a number of columns have a very high percentage of NA values especially in the Beer datasets. Analysis on these columns would be limited to a very narrow portion of the dataset, so we decide to drop these columns instead. We decide to drop columns that have more than 60% of NA values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:47.204283Z",
     "start_time": "2022-12-22T16:20:46.942617Z"
    }
   },
   "outputs": [],
   "source": [
    "df_adv_beer = df_adv_beer[df_adv_beer.columns[df_adv_beer.isna().sum()/df_adv_beer.shape[0] < 0.60]]\n",
    "df_rb_beer=df_rb_beer[df_rb_beer.columns[df_rb_beer.isna().sum()/df_rb_beer.shape[0] < 0.60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:47.222170Z",
     "start_time": "2022-12-22T16:20:47.205761Z"
    }
   },
   "outputs": [],
   "source": [
    "#We check that all beers are unique in both datasets\n",
    "print(df_adv_beer[\"beer_id\"].nunique()==df_adv_beer.shape[0])\n",
    "print(df_rb_beer[\"beer_id\"].nunique()==df_rb_beer.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:47.530904Z",
     "start_time": "2022-12-22T16:20:47.455655Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some beers don't have ratings. We are not interested in them, so we drop them\n",
    "df_adv_beer_wrating=df_adv_beer.drop(df_adv_beer[df_adv_beer['nbr_ratings']==0].index)\n",
    "print(\"{} beers of the Beer Advocate dataset have been dropped\".format((df_adv_beer.shape[0]-df_adv_beer_wrating.shape[0])))\n",
    "df_rb_beer_wrating=df_rb_beer.drop(df_rb_beer[df_rb_beer['nbr_ratings']==0].index)\n",
    "print(\"{} beers of the RateBeer dataset have been dropped\".format((df_rb_beer.shape[0]-df_rb_beer_wrating.shape[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='user_processing'></a>\n",
    "\n",
    "### Users datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:47.992975Z",
     "start_time": "2022-12-22T16:20:47.977732Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's have a look \n",
    "df_adv_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:48.335518Z",
     "start_time": "2022-12-22T16:20:48.316985Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:48.622503Z",
     "start_time": "2022-12-22T16:20:48.614136Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's check the shape of the datasets\n",
    "print(\"Beer advocate users dataset has {} rows and {} colomns\".format(df_adv_users.shape[0],df_adv_users.shape[1]))\n",
    "print(\"RateBeer users dataset has {} rows and {} colomns\".format(df_rb_users.shape[0],df_rb_users.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:48.933330Z",
     "start_time": "2022-12-22T16:20:48.845219Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some user ids are in duplicates in the users datasets. We drop identical user ids to have only one instance of each.\n",
    "df_adv_unique_users=df_adv_users.drop_duplicates(subset=\"user_id\",keep=\"first\") #suppresses all copies of same user_id and keeps the first instance\n",
    "#sanity check \n",
    "print(df_adv_unique_users.shape[0]==df_adv_unique_users[\"user_id\"].nunique())\n",
    "df_rb_unique_users=df_rb_users.drop_duplicates(subset=\"user_id\",keep=\"first\") \n",
    "#sanity check \n",
    "print(df_rb_unique_users.shape[0]==df_rb_unique_users[\"user_id\"].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='brewery_processing'></a>\n",
    "\n",
    "### Breweries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:49.330390Z",
     "start_time": "2022-12-22T16:20:49.320937Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's check the shape of the breweries dataset\n",
    "print(\"Beer advocate breweries dataset has {} rows and {} colomns\".format(df_adv_breweries.shape[0],df_adv_breweries.shape[1]))\n",
    "print(\"RateBeer breweries dataset has {} rows and {} colomns\".format(df_rb_breweries.shape[0],df_rb_breweries.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:50.019180Z",
     "start_time": "2022-12-22T16:20:50.004132Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's have a look\n",
    "df_adv_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:50.628197Z",
     "start_time": "2022-12-22T16:20:50.611863Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_breweries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:51.132455Z",
     "start_time": "2022-12-22T16:20:51.110475Z"
    }
   },
   "outputs": [],
   "source": [
    "#We don't care about breweries that don't have any beers, so we drop them \n",
    "df_adv_breweries_wbeer=df_adv_breweries.drop(df_adv_breweries[df_adv_breweries['nbr_beers']==0].index)\n",
    "print(\"{} breweries have been dropped from the Beer Advocate dataset\".format((df_adv_breweries.shape[0]-df_adv_breweries_wbeer.shape[0])))\n",
    "df_rb_breweries_wbeer=df_rb_breweries.drop(df_rb_breweries[df_rb_breweries['nbr_beers']==0].index)\n",
    "df_rb_breweries_wbeer['nbr_beers'].sort_values(ascending=True)\n",
    "print(\"{} breweries have been dropped from the RateBeer dataset\".format((df_rb_breweries.shape[0]-df_rb_breweries_wbeer.shape[0])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "\n",
    "## Exploration of the datasets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:54.188227Z",
     "start_time": "2022-12-22T16:20:52.862852Z"
    }
   },
   "outputs": [],
   "source": [
    "#Histograms\n",
    "#Beers binned by ratings count:\n",
    "quantiles = [0.25, 0.5, 0.75]\n",
    "\n",
    "titles = ['Beer Advocate - ratings by beer','Rate Beer - ratings by beer']\n",
    "plot_data = [df_adv_beer_wrating.nbr_ratings, df_rb_beer_wrating.nbr_ratings]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 7),sharey=True,sharex=True)\n",
    "fig.tight_layout(pad=5)\n",
    "for i,datum in enumerate(plot_data):\n",
    "    sns.histplot(ax=axes.flat[i],data=datum,bins=100,log_scale=(True,True), kde=False, color=\"blue\")\n",
    "    axes.flat[i].set_xlabel(\"Beer count\")\n",
    "    axes.flat[i].set_ylabel(\"Rating Count\")\n",
    "    axes.flat[i].set_title(titles[i], pad=20)\n",
    "    for q in quantiles:\n",
    "        axes.flat[i].axvline(plot_data[i].quantile(q), 0, 1, color=\"black\", ls='--',linewidth=3)\n",
    "        axes.flat[i].text(plot_data[i].quantile(q)+0.1, 2e5, str(int(q*100))+'%', horizontalalignment='left', size='medium', color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a skewed distribution of beers according to the rating count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:54.232579Z",
     "start_time": "2022-12-22T16:20:54.190180Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some stats\n",
    "print(df_adv_beer_wrating.shape)\n",
    "df_adv_beer_wrating[[\"nbr_ratings\",\"avg\",\"abv\",\"avg_computed\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:54.301428Z",
     "start_time": "2022-12-22T16:20:54.234771Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_rb_beer_wrating.shape)\n",
    "df_rb_beer_wrating[[\"nbr_ratings\",\"avg\",\"abv\",\"avg_computed\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:55.349570Z",
     "start_time": "2022-12-22T16:20:54.303358Z"
    }
   },
   "outputs": [],
   "source": [
    "#Users binned by rating count\n",
    "\n",
    "titles = ['Beer Advocate - ratings by user','Rate Beer - ratings by user', 'Matched RateBeer - ratings by user','Matched Beer Advocate - ratings by user']\n",
    "plot_data = [df_rb_unique_users.nbr_ratings, df_adv_unique_users.nbr_ratings]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 7),sharey=True,sharex=True)\n",
    "fig.tight_layout(pad=5)\n",
    "for i,datum in enumerate(plot_data):\n",
    "    sns.histplot(ax=axes.flat[i],data=datum,bins=100,log_scale=(True,True), kde=False,color=\"blue\")\n",
    "    axes.flat[i].set_xlabel(\"Rating count\")\n",
    "    axes.flat[i].set_ylabel(\"User Count\")\n",
    "    axes.flat[i].set_title(titles[i], pad=20)\n",
    "    for q in quantiles:\n",
    "        #Offset of 0.1 so it is more readable\n",
    "        axes.flat[i].axvline(plot_data[i].quantile(q)+0.1, 0, 1, color=\"black\", ls='--',linewidth=3)\n",
    "        axes.flat[i].text(plot_data[i].quantile(q)+0.1, 1e5, str(int(q*100))+'%', horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:41:05.847677Z",
     "start_time": "2022-11-18T14:41:05.844299Z"
    }
   },
   "source": [
    "We observe a skewed distribution of the number of ratings according to the number of raters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:55.360416Z",
     "start_time": "2022-12-22T16:20:55.350772Z"
    }
   },
   "outputs": [],
   "source": [
    "#Summary statistics of users\n",
    "print(df_adv_unique_users.shape)\n",
    "df_adv_unique_users[[\"nbr_ratings\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:55.368753Z",
     "start_time": "2022-12-22T16:20:55.361373Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_rb_unique_users.shape)\n",
    "df_rb_unique_users[[\"nbr_ratings\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:55.381561Z",
     "start_time": "2022-12-22T16:20:55.370535Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of different locations the users come from: \n",
    "print(\"The users of Beer Advocate come from {} different locations\".format(df_adv_unique_users[\"location\"].nunique()))\n",
    "print(\"The users of RateBeer come from {} different locations\".format(df_rb_unique_users[\"location\"].nunique()))\n",
    "#Location can either be countries, national regions (case of UK) and states (case of USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:55.397869Z",
     "start_time": "2022-12-22T16:20:55.382536Z"
    }
   },
   "outputs": [],
   "source": [
    "#Top 5 locations of the users for each dataset:\n",
    "users_top5_locations=pd.DataFrame(columns=['Beer Advocate users','Rate Beer users'])\n",
    "users_top5_locations['Beer Advocate users']=df_adv_unique_users[\"location\"].value_counts().index.tolist()[:5] #we sort the number of occurences of each location and extract the corresponding top 5 locations\n",
    "users_top5_locations['Rate Beer users']=df_rb_unique_users[\"location\"].value_counts().index.tolist()[:5]\n",
    "\n",
    "users_top5_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breweries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:56.443864Z",
     "start_time": "2022-12-22T16:20:55.709271Z"
    }
   },
   "outputs": [],
   "source": [
    "#Histograms\n",
    "#Breweries binned by beer count\n",
    "\n",
    "titles = ['Beer Advocate - beers by brewery','Rate Beer - beers by brewery']\n",
    "plot_data = [df_rb_breweries_wbeer.nbr_beers, df_adv_breweries_wbeer.nbr_beers]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 7),sharey=True,sharex=True)\n",
    "fig.tight_layout(pad=5)\n",
    "for i,datum in enumerate(plot_data):\n",
    "    sns.histplot(ax=axes.flat[i],data=datum,bins=100,log_scale=(True,True), kde=False, color=\"blue\")\n",
    "    axes.flat[i].set_xlabel(\"Brewery count\")\n",
    "    axes.flat[i].set_ylabel(\"Beer Count\")\n",
    "    axes.flat[i].set_title(titles[i], pad=20)\n",
    "    for q in quantiles:\n",
    "        axes.flat[i].axvline(plot_data[i].quantile(q), 0, 1, color=\"black\", ls='--',linewidth=3)\n",
    "        axes.flat[i].text(plot_data[i].quantile(q)+0.1, 4e3, str(int(q*100))+'%', horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a skewed distribution of the number of breweries according to their beer count. In addition, we notice that the RateBeer dataset has no breweries with more than 200 beers. It is possible that some data was lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:56.796517Z",
     "start_time": "2022-12-22T16:20:56.776833Z"
    }
   },
   "outputs": [],
   "source": [
    "#Some stats\n",
    "print(df_adv_breweries_wbeer.shape)\n",
    "df_adv_breweries_wbeer[[\"nbr_beers\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:57.548854Z",
     "start_time": "2022-12-22T16:20:57.528066Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_rb_breweries_wbeer.shape)\n",
    "df_rb_breweries_wbeer[[\"nbr_beers\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:58.247366Z",
     "start_time": "2022-12-22T16:20:58.234051Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of different locations the breweries come from: \n",
    "print(\"The breweries of Beer Advocate come from {} different locations\".format(df_adv_breweries_wbeer[\"location\"].nunique()))\n",
    "print(\"The breweries of RateBeer come from {} different locations\".format(df_rb_breweries_wbeer[\"location\"].nunique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:20:59.096784Z",
     "start_time": "2022-12-22T16:20:59.072697Z"
    }
   },
   "outputs": [],
   "source": [
    "#Top 5 locations of the breweries for each dataset:\n",
    "breweries_top5_locations=pd.DataFrame(columns=['Beer Advocate breweries','Rate Beer breweries'])\n",
    "breweries_top5_locations['Beer Advocate breweries']=df_adv_breweries_wbeer[\"location\"].value_counts().index.tolist()[:5] #we sort the number of occurences of each location and extract the corresponding top 5 locations\n",
    "breweries_top5_locations['Rate Beer breweries']=df_rb_breweries_wbeer[\"location\"].value_counts().index.tolist()[:5]\n",
    "\n",
    "breweries_top5_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textual_reviews'></a>\n",
    "## Textual reviews and rating analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:54:39.787774Z",
     "start_time": "2022-12-21T19:54:39.784029Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we use a utilitary function to directly compute everything:\n",
    "from functions import NLP_utils\n",
    "\n",
    "help(NLP_utils.summary_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:55:45.655611Z",
     "start_time": "2022-12-21T19:54:40.204039Z"
    }
   },
   "outputs": [],
   "source": [
    "RB_counts, RB_dates = NLP_utils.summary_analysis(\"RateBeer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:56:40.922730Z",
     "start_time": "2022-12-21T19:55:45.661147Z"
    }
   },
   "outputs": [],
   "source": [
    "BA_counts, BA_dates = NLP_utils.summary_analysis(\"BeerAdvocate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:57:28.371763Z",
     "start_time": "2022-12-21T19:56:40.926143Z"
    }
   },
   "outputs": [],
   "source": [
    "# How does the review rate of users changes with time for each site:\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 7),sharey=True,sharex=True)\n",
    "RB_dates = pd.to_datetime(RB_dates,unit='s')\n",
    "BA_dates = pd.to_datetime(BA_dates,unit='s')\n",
    "axes[1].set_title(\"Time evolution of RateBeer review posting\", pad=20)\n",
    "sns.histplot(RB_dates,log_scale=(False,False),kde=True,ax=axes[0])\n",
    "axes[0].set_title(\"Time evolution of BeerAdvocate review posting\", pad=20)\n",
    "sns.histplot(BA_dates,log_scale=(False,False),kde=True,ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for both datasets that the review posting shows a trend of increasing with time during the 2000s. However, while the review posting seems to keep increasing for BeerAdvocate, we see a decrease in the case of RateBeer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:57:33.804022Z",
     "start_time": "2022-12-21T19:57:28.378264Z"
    }
   },
   "outputs": [],
   "source": [
    "#Word count distributions for each site: \n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 7),sharey=True,sharex=False)\n",
    "\n",
    "axes[1].set_title('Rate Beer - Review word count')\n",
    "axes[1].set_xlabel('Word per review')\n",
    "axes[1].set_ylabel('Review count')\n",
    "sns.histplot(RB_counts,bins=100, log_scale=(True,False),ax=axes[0])\n",
    "\n",
    "axes[0].set_title('Beer Advocate - Review word count')\n",
    "axes[0].set_xlabel('Word per review')\n",
    "axes[0].set_ylabel('Review count')\n",
    "sns.histplot(BA_counts,bins=100, log_scale=(True,False),ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:57:34.763120Z",
     "start_time": "2022-12-21T19:57:33.805735Z"
    }
   },
   "outputs": [],
   "source": [
    "#It looks like the two distributions might be nomal. Let's check:\n",
    "print(diagnostic.kstest_normal(BA_counts.values, dist = 'norm'))\n",
    "print(diagnostic.kstest_normal(RB_counts.values, dist = 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reject the null hypothesis that the distribution of words per review of the Beer Advocate dataset follows a normal distribution. We get NaN values for the test on distribution of words per reviews of the Rate Beer dataset. We suppose that the distribution was so far from normal that it did not work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:58:59.730501Z",
     "start_time": "2022-12-21T19:57:34.764818Z"
    }
   },
   "outputs": [],
   "source": [
    "#We start by fetching the ratings\n",
    "help(NLP_utils.fetch_ratings)\n",
    "rb_ratings = NLP_utils.fetch_ratings(\"RateBeer\")\n",
    "ba_ratings = NLP_utils.fetch_ratings(\"BeerAdvocate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:07.896410Z",
     "start_time": "2022-12-21T19:58:59.734366Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ratings distribution for each site\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 7),sharey=True,sharex=False)\n",
    "\n",
    "axes[1].set_title('Rate beer - Rating distribution') \n",
    "axes[1].set_xlabel('Ratings')\n",
    "axes[1].set_ylabel('Ratings count')\n",
    "sns.histplot(rb_ratings,bins=100, log_scale=(False,False),ax=axes[0])\n",
    "\n",
    "axes[0].set_title('Beer Advocate - Rating distribution')\n",
    "axes[0].set_xlabel('Ratings')\n",
    "axes[0].set_ylabel('Ratings count')\n",
    "sns.histplot(ba_ratings,bins=100, log_scale=(False,False),ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.123951Z",
     "start_time": "2022-12-21T19:59:07.898383Z"
    }
   },
   "outputs": [],
   "source": [
    "#It looks like the first distribution might be normal. The second is most likely not (looks like users like giving\n",
    "#marks that are round numbers more than fractional marks), but let's check! @Auriane: je ne comprends pas ce graphe.\n",
    "print(diagnostic.kstest_normal(ba_ratings.values, dist = 'norm'))\n",
    "print(diagnostic.kstest_normal(rb_ratings.values, dist = 'norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we reject the null hypothesis that the distributions are normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing for bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.194889Z",
     "start_time": "2022-12-21T19:59:09.125542Z"
    }
   },
   "outputs": [],
   "source": [
    "#To facilitate our analysis, we add a column in the beer dataset corresponding to the location of the brewery.\n",
    "location_to_brewery_name_adv=dict(zip(df_adv_breweries_wbeer.name,df_adv_breweries_wbeer.location)) #create a dictionary which keys are the breweries name and values are the breweries location\n",
    "df_adv_beer_wrating[\"location\"]=df_adv_beer_wrating.brewery_name.map(location_to_brewery_name_adv) #create a new location column indicating the location of the brewery\n",
    "df_adv_beer_wrating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.280622Z",
     "start_time": "2022-12-21T19:59:09.198899Z"
    }
   },
   "outputs": [],
   "source": [
    "location_to_brewery_name_rb=dict(zip(df_rb_breweries_wbeer.name,df_rb_breweries_wbeer.location)) #create a dictionary which keys are the breweries name and values are the breweries location\n",
    "df_rb_beer_wrating[\"location\"]=df_rb_beer_wrating.brewery_name.map(location_to_brewery_name_rb) #create a new location column indicating the location of the brewery\n",
    "df_rb_beer_wrating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on the best beers before applying corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.476314Z",
     "start_time": "2022-12-21T19:59:09.281823Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we sort beers depending on their average rating for both datasets.  \n",
    "df_beer_absolute_adv=df_adv_beer_wrating.sort_values(by=\"avg\",ascending=False).reset_index()\n",
    "df_beer_absolute_adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.753560Z",
     "start_time": "2022-12-21T19:59:09.477626Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we sort beers depending on their average rating for both datasets.  \n",
    "df_beer_absolute_rb=df_rb_beer_wrating.sort_values(by=\"avg\",ascending=False).reset_index()\n",
    "df_beer_absolute_rb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:09.899975Z",
     "start_time": "2022-12-21T19:59:09.754663Z"
    }
   },
   "outputs": [],
   "source": [
    "#We want to extract the best beer brewed in each location\n",
    "grouped_adv_beer=df_adv_beer_wrating.sort_values([\"location\",'avg'],ascending=False).groupby('location').head(1) #groups beers coming from beweries of the same location\n",
    "top_adv_country=df_adv_beer_wrating.sort_values([\"location\",'avg'],ascending=False).groupby('location').head(1).reset_index(drop=True) #gives top beer for each location! \n",
    "top_adv_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:10.098342Z",
     "start_time": "2022-12-21T19:59:09.901062Z"
    }
   },
   "outputs": [],
   "source": [
    "#We want to extract the best beer brewed in each location\n",
    "grouped_rb_beer=df_rb_beer_wrating.sort_values([\"location\",'avg'],ascending=False).groupby('location').head(1) #groups beers coming from beweries of the same location\n",
    "top_rb_country=df_rb_beer_wrating.sort_values([\"location\",'avg'],ascending=False).groupby('location').head(1).reset_index(drop=True) #gives top beer for each location! \n",
    "top_rb_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:10.113415Z",
     "start_time": "2022-12-21T19:59:10.099504Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now that we have the best beer brewed in each location, let's rank locations\n",
    "top_ranked_adv_country=top_adv_country.sort_values(by=\"avg\",ascending=False).reset_index()\n",
    "top_ranked_adv_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:59:10.127103Z",
     "start_time": "2022-12-21T19:59:10.114893Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now that we have the best beer brewed in each location, let's rank locations\n",
    "top_ranked_rb_country=top_rb_country.sort_values(by=\"avg\",ascending=False).reset_index()\n",
    "top_ranked_rb_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:21:34.893925Z",
     "start_time": "2022-12-22T16:21:34.873959Z"
    }
   },
   "outputs": [],
   "source": [
    "#Computation of the average rating of the beers\n",
    "rb_average_rating=df_rb_beer.avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:25:46.678589Z",
     "start_time": "2022-12-22T16:21:35.359910Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "help(NLP_utils.debiasing)\n",
    "NLP_utils.debiasing(\"RateBeer\",df_rb_beer_wrating, df_rb_unique_users)\n",
    "NLP_utils.debiasing(\"BeerAdvocate\",df_adv_beer_wrating,df_adv_unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:25:46.705965Z",
     "start_time": "2022-12-22T16:25:46.685462Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_adv_beer_wrating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:39.342478Z",
     "start_time": "2022-12-21T20:03:38.061037Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ratings distribution for each site\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    \"\"\"Erases duplicates in legend handles before plotting\"\"\"\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "\n",
    "#Plots of results of debiasing    \n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 7),sharey=True,sharex=False)\n",
    "axes[0].set_title('Rate Beer - Distribution of ratings')\n",
    "axes[0].set_xlabel('Ratings')\n",
    "axes[0].set_ylabel('Ratings count')\n",
    "sns.histplot(df_rb_beer_wrating[\"avg\"],bins=100, log_scale=(False,False),ax=axes[0],color='red',label=\"initial average rating\")\n",
    "sns.histplot(df_rb_beer_wrating[\"debiased_avg\"],bins=100, log_scale=(False,False),ax=axes[0],label=\"debiased average rating\")\n",
    "\n",
    "axes[1].set_title('Beer Advocate - Distribution of ratings')\n",
    "axes[1].set_xlabel('Ratings')\n",
    "axes[1].set_ylabel('Ratings count')\n",
    "sns.histplot(df_adv_beer_wrating[\"avg\"],bins=100, log_scale=(False,False),ax=axes[1],color='red',label=\"initial average rating\")\n",
    "sns.histplot(df_adv_beer_wrating[\"debiased_avg\"],bins=100, log_scale=(False,False),ax=axes[1],label=\"debiased average rating\")\n",
    "legends = [legend_without_duplicate_labels(ax) for ax in axes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beer characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:39.381080Z",
     "start_time": "2022-12-21T20:03:39.344566Z"
    }
   },
   "outputs": [],
   "source": [
    "Anglo_American_Ales=['Altbier', 'Barley Wine',\"Bitter\",'Premium Bitter/ESB',\"Golden Ale/Blond Ale\",\"Brown Ale\", \"California Common\",\"Cream Ale\",\"Black IPA\",\"India Pale Ale (IPA)\",\"Imperial IPA\",\"Session IPA\",\"Kölsch\",\"American Pale Ale\",\"Irish Ale\",\"English Strong Ale\", \"American Strong Ale\",\"Mild Ale\",\"Amber Ale\",\"English Pale Ale\",\"Traditional ALe\",\"Scotch Ale\",\"Old Ale\",\"Scottish Ale\"]\n",
    "Beligan_Style_Ales=[\"Belgian Ale\",\"Belgian Strong Ale\",\"Bière de Garde\",\"Abbey Dubbel\",'Abt/Quadrupel',\"Saison\",\"Abbey Tripel\"]\n",
    "Lagers=[\"Pale Lager\",\"Premium Lager\",\"Imperial Pils/Strong Pale Lager\",\"India Style Lager\",\"Amber Lager/Vienna\",'Czech Pilsner (Světlý)',\"Pilsener\",\"Heller Bock\",\"Doppelbock\",\"Dumbler Bock\",\"Weizen Bock\",\"Esibock\",\"Malt Liquor\",\"Oktoberfest/Märzen\",\"Radler/Shandy\",\"Zwickel/Keller/Landbier\",\"Dortmunder/Helles\",'Dunkel/Tmavý','Schwarzbier','Polotmavý']\n",
    "Stout_and_Porter=[\"Stout\",\"Imperial Stout\",\"Foreign Stout\",\"Sweet Stout\",\"Dry Stout\",\"Porter\",\"Baltic Porter\",\"Imperial Porter\"]\n",
    "Wheat_beer=[\"Wheat Ale\",\"Witbier\",'German Hefeweizen','Dunkelweizen','German Kristallweizen']\n",
    "Sour_beer=[\"Berliner Weisse\",\"Sour/Wild Ale\",\"Sour Red/Brown\",'Grodziskie/Gose/Lichtenhainer','Lambic Style - Gueuze', 'Lambic Style - Unblended','Lambic Style - Faro','Lambic Style - Fruit',\"Grodziskie/Gose/Lichtenhainer\"]\n",
    "Other_styles=[\"Spice/Herb/Vegetable\",\"Smoked\",'Fruit Beer',\"Sahti/Gotlandsdricke/Koduõlu\",'Low Alcohol','Specialty Grain']\n",
    "Cider_Mead_Saké=['Cider','Mead','Saké - Daiginjo', 'Saké - Namasaké','Saké - Ginjo', 'Saké - Infused', 'Saké - Tokubetsu','Saké - Junmai', 'Saké - Nigori', 'Saké - Koshu', 'Saké - Taru','Saké - Honjozo', 'Saké - Genshu', 'Saké - Futsu-shu','Perry']\n",
    "beer_style_dict={key: \"Anglo American Ales\" for key in Anglo_American_Ales}|{key: \"Belgian Style Ales\" for key in Beligan_Style_Ales}|{key:\"Lagers\" for key in Lagers}|{key:\"Stout and Porter\" for key in Stout_and_Porter}|{key:\"Wheat beer\" for key in Wheat_beer}|{key:\"Sour beer\" for key in Sour_beer}|{key:\"Other styles\" for key in Other_styles}|{key:\"Cider, Mead and Saké\" for key in Cider_Mead_Saké}\n",
    "\n",
    "\n",
    "df_rb_beer_wrating[\"streamline_style\"]=df_rb_beer_wrating[\"style\"].map(beer_style_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:40.170026Z",
     "start_time": "2022-12-21T20:03:39.385275Z"
    }
   },
   "outputs": [],
   "source": [
    "states=['United States, Wisconsin', 'United States, Georgia',\n",
    "    'United States, North Carolina', 'United States, Arkansas',\n",
    "       'United States, Louisiana', 'United States, West Virginia',\n",
    "       'United States, California', 'United States, Washington',\n",
    "       'United States, Massachusetts', 'United States, New Jersey',\n",
    "       'United States, Maryland', 'United States, Arizona',\n",
    "       'United States, Pennsylvania', 'United States, Indiana',\n",
    "       'United States, Montana', 'United States, South Dakota',\n",
    "       'United States, Tennessee', 'United States, Mississippi',\n",
    "       'United States, Virginia', 'United States, Missouri',\n",
    "       'United States, Maine', 'United States, Alabama',\n",
    "       'United States, New Hampshire', 'United States, Delaware',\n",
    "       'United States, Iowa', 'United States, Minnesota',\n",
    "       'United States, Kentucky', 'United States, Nebraska',\n",
    "       'United States, Wyoming', 'United States, Vermont',\n",
    "       'United States, New Mexico', 'United States, Alaska',\n",
    "       'United States, Rhode Island', 'United States, Kansas',\n",
    "       'United States, Idaho', 'United States, Washington DC',\n",
    "       'United States, Ohio', 'United States, Michigan',\n",
    "       'United States, North Dakota', 'United States, Nevada',\n",
    "       'United States, Oregon', 'United States, Hawaii',\n",
    "       'United States, Connecticut', 'United States, Texas',\n",
    "       'United States, Illinois', 'United States, South Carolina',\n",
    "       'United States, Oklahoma', 'United States, Utah','United States, Florida','United States, Colorado','United States, New York']\n",
    "kingdoms=[\"England\",'Northern Ireland',\"Scotland\",\"Wales\"]\n",
    "\n",
    "country_dict={key:\"United Kingdom of Great Britain and Northern Ireland\" for key in kingdoms}|{key:\"United States of America\" for key in states}|{'Virgin Islands (British)':\"United Kingdom of Great Britain and Northern Ireland\",'Northern Marianas':'Northern Mariana Islands','South Ossetia':'Georgia','Dem Rep of Congo':'Congo','Nagorno-Karabakh':'Azerbaijan','Transdniestra':'Moldova','Saint Vincent and The Grenadines':'Saint Vincent and the Grenadines','Trinidad & Tobago':'Trinidad and Tobago',\"Kosovo\":\"Albania\",\"Reunion\":\"Réunion\",\"Virgin Islands (U.S.)\":\"United States of America\",'Tibet':\"China\",\"Abkhazia\":\"Georgia\",'Cape Verde Islands':'Cabo Verde',\"Fiji Islands\":\"Fiji\",'Turkish Republic of Cyprus':'Cyprus','Antigua & Barbuda':'Antigua and Barbuda'}\n",
    "\n",
    "df_rb_beer_wrating[\"Country\"]=df_rb_beer_wrating.location.replace(country_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:40.924749Z",
     "start_time": "2022-12-21T20:03:40.171318Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_beer_wrating[\"Country\"]=df_rb_beer_wrating.location.replace(country_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:41.027175Z",
     "start_time": "2022-12-21T20:03:40.925770Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_beer_wrating.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:41.369566Z",
     "start_time": "2022-12-21T20:03:41.028191Z"
    }
   },
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "def country_to_continent(country_name):\n",
    "    country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "    if country_alpha2==\"TL\":\n",
    "        country_continent_name=\"Asia\"\n",
    "    else:\n",
    "        country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "        country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "    return country_continent_name\n",
    "\n",
    "df_rb_beer_wrating[\"Continent\"]=df_rb_beer_wrating[\"Country\"].apply(country_to_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:41.584587Z",
     "start_time": "2022-12-21T20:03:41.370697Z"
    }
   },
   "outputs": [],
   "source": [
    "beer_features=pd.DataFrame(df_rb_beer_wrating[[\"abv\",\"streamline_style\",\"Continent\"]])\n",
    "beer_features.dropna(inplace=True)\n",
    "categorical_columns=[\"streamline_style\",\"Continent\"]\n",
    "beer_features=pd.get_dummies(beer_features, columns=categorical_columns)\n",
    "beer_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:41.588345Z",
     "start_time": "2022-12-21T20:03:41.585747Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:42.386696Z",
     "start_time": "2022-12-21T20:03:41.594858Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_features = StandardScaler().fit(beer_features).transform(beer_features)\n",
    "X_reduced_PCA = PCA(n_components=2,random_state=0).fit_transform(scaled_features)\n",
    "\n",
    "plt.scatter(X_reduced_PCA[:,0], X_reduced_PCA[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:42.660605Z",
     "start_time": "2022-12-21T20:03:42.387877Z"
    }
   },
   "outputs": [],
   "source": [
    "mypca=PCA(n_components=2,random_state=0)\n",
    "mypca.fit_transform(scaled_features)\n",
    "mypca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:03:42.665495Z",
     "start_time": "2022-12-21T20:03:42.661928Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=10, end=20):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10,n_init=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "    sse=pd.DataFrame(sse)\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:00.962591Z",
     "start_time": "2022-12-21T20:03:42.666826Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_sse(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:06.856790Z",
     "start_time": "2022-12-21T20:04:00.965096Z"
    }
   },
   "outputs": [],
   "source": [
    "labels=0\n",
    "cluster = KMeans(n_clusters=13, random_state=0,n_init=10).fit(scaled_features)\n",
    "plt.scatter(X_reduced_PCA[:,0], X_reduced_PCA[:,1],c=cluster.labels_,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:06.999498Z",
     "start_time": "2022-12-21T20:04:06.858586Z"
    }
   },
   "outputs": [],
   "source": [
    "mypca=PCA(n_components=15,random_state=0)\n",
    "mypca.fit_transform(scaled_features)\n",
    "mypca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:07.004329Z",
     "start_time": "2022-12-21T20:04:07.000914Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cluster.labels_)\n",
    "#labels.labels_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:07.195641Z",
     "start_time": "2022-12-21T20:04:07.005974Z"
    }
   },
   "outputs": [],
   "source": [
    "#beer_features=pd.DataFrame(df_rb_beer_wrating[[\"abv\",\"streamline_style\",\"Continent\"]])\n",
    "#beer_features.dropna(inplace=True)\n",
    "df_rb_beer_wrating_test=df_rb_beer_wrating.dropna(subset=[\"abv\",\"streamline_style\",\"Continent\"])\n",
    "df_rb_beer_wrating_test.head()\n",
    "#df_rb_beer_wrating_test[\"cluster\"]=cluster.labels_\n",
    "#df_rb_beer_wrating_test[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:07.203384Z",
     "start_time": "2022-12-21T20:04:07.197399Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_beer_wrating_test[\"cluster\"]=cluster.labels_\n",
    "df_rb_beer_wrating_test[\"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:07.210318Z",
     "start_time": "2022-12-21T20:04:07.204493Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_beer_wrating_test[\"cluster\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:07.266666Z",
     "start_time": "2022-12-21T20:04:07.214148Z"
    }
   },
   "outputs": [],
   "source": [
    "rb_grouped_by_cluster=df_rb_beer_wrating_test.groupby([\"cluster\"]).debiased_avg\n",
    "rb_grouped_by_cluster.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:04:09.844413Z",
     "start_time": "2022-12-21T20:04:07.268109Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rb_beer_wrating_test.groupby([\"cluster\"]).boxplot(column=['debiased_avg'],subplots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive and Negative words analysis in the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis we worked with csv files, which allowed  more flexibility and the possible to run the code only once. We divided the AdvocateBeer reviews into 10 csv files with a maximum of 250,000 data, and 28 csv files with a maximum of 250,000 data for RateBeer. For each CSV we calculated the average number of positive and negative words per country. Theresults generated are small ( in ko)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lines of code below compute the average of positifs and negatis words for each csv per country and put the results\n",
    "# in the folder generated data\n",
    "# This line of code just need to run one time to get the csv file\n",
    "# the durtion of run can take multiple hours\n",
    "\n",
    "#loop_RateBeer_get_pos_neg_words()\n",
    "#loop_AdvBeer_get_pos_neg_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate Beer Management\n",
    "\n",
    "# This function merge the results of all the csv files generated above\n",
    "df_all_rb =  ratebeer_merging_csv_results()\n",
    "\n",
    "# We group by country with all dataframe concated to get a dataframe with all the resume o the analysis to send a csv\n",
    "# to add the information in the map\n",
    "country_res_groups_all_rb = df_all_rb.groupby(by='country').apply(weighted_average_all, 'pos_words','neg_words','nb_review')\n",
    "df_final_rb = pd.DataFrame(country_res_groups_all_rb.tolist(), columns=[['neg_words','pos_words','nb_review']])\n",
    "df_final_rb['location'] = country_res_groups_all_rb.index\n",
    "\n",
    "# save the results in a csv file to ease the integration in the worlds map\n",
    "df_final_rb.to_csv(rateBeer_root + 'RateBeer_pos_neg_analysis_resume.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advocate Beer Management\n",
    "\n",
    "# This function merge the results of all the csv files generated above\n",
    "df_all_adv =  advbeer_merging_csv_results()\n",
    "\n",
    "# We group by country with all dataframe concated to get a dataframe with all the resume o the analysis to send a csv\n",
    "# to add the information in the map\n",
    "country_res_groups_all_adv = df_all_adv.groupby(by='country').apply(weighted_average_all, 'pos_words','neg_words','nb_review')\n",
    "df_final_adv = pd.DataFrame(country_res_groups_all_adv.tolist(), columns=[['neg_words','pos_words','nb_review']])\n",
    "df_final_adv['location'] = country_res_groups_all_adv.index\n",
    "\n",
    "# save the results in a csv file to ease the integration in the worlds map\n",
    "df_final_adv.to_csv(advBeer_root + 'BeerAdv_pos_neg_analysis_resume.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching SAT Beers with RateBeer/BeerAdvocate - Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:33:19.346557Z",
     "start_time": "2022-12-22T16:29:12.696622Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "SAT_beers = read_data.fetch_satellite_df()\n",
    "BA_beers = df_adv_beer_wrating\n",
    "RB_beers = df_rb_beer_wrating\n",
    "BA_beers = BA_beers[BA_beers[\"nbr_ratings\"] != 0].copy()\n",
    "RB_beers = RB_beers[RB_beers[\"nbr_ratings\"] != 0].copy()\n",
    "sat_queries = NLP_utils.querify(SAT_beers)\n",
    "\n",
    "\n",
    "#The queries should have the same document structure as the corpus. \n",
    "for index, biere in SAT_beers.iterrows():\n",
    "    sat_queries[index] = (NLP_utils.tokenize(biere['nom']) +\" \"  + NLP_utils.tokenize(biere['brasseur']) + \" \" + NLP_utils.tokenize(str(biere['alcool'])))\n",
    "\n",
    "#We implemented Vector Space Retrieval with cosine similarities to find matches between SAT and datasets\n",
    "#More information can be found in the docstring:\n",
    "help(NLP_utils.vector_space_retrieval)\n",
    "BA_results = NLP_utils.vector_space_retrieval(sat_queries,BA_beers,k=5)\n",
    "RB_results =  NLP_utils.vector_space_retrieval(sat_queries,RB_beers,k=5)\n",
    "#We rename columns to avoid mixing up results when finding ratings of SAT beers.\n",
    "BA_results.columns = 'BA_' + BA_results.columns.values\n",
    "RB_results.columns = 'RB_' + RB_results.columns.values\n",
    "beer_retrieval_results = pd.concat([pd.DataFrame(np.repeat(SAT_beers[[\"nom\",\"alcool\"]].values, 5, axis=0),columns=[\"nom\",\"alcool\"]),BA_results.reset_index()[[\"BA_beer_name\",\"BA_avg\",'BA_similarity','BA_abv', 'BA_brewery_name', 'BA_style', 'BA_beer_id']],RB_results.reset_index()[[\"RB_beer_name\",\"RB_avg\",'RB_similarity','RB_abv', 'RB_brewery_name', 'RB_style', 'RB_beer_id']]],axis=1)\n",
    "#We save our work\n",
    "beer_retrieval_results.to_csv(\"DATA/matched_SATbeers.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting ratings of matched beers and estimating ratings of not matched beers - Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:34:12.436355Z",
     "start_time": "2022-12-22T16:34:11.695431Z"
    }
   },
   "outputs": [],
   "source": [
    "#We also created a helper function to automatically match beers of the dataset we found with Retrieval following our matching heuristic.\n",
    "# More information can be found with help().\n",
    "help(SAT_helpers.generate_automatic_beer_matches)\n",
    "SAT_match_candidates = beer_retrieval_results\n",
    "\n",
    "automatic_matches_BA, not_matched_BA, manual_match_BA = SAT_helpers.generate_automatic_beer_matches(\"BeerAdvocate\",SAT_match_candidates,df_adv_beer_wrating)\n",
    "automatic_matches_RB, not_matched_RB, manual_match_RB = SAT_helpers.generate_automatic_beer_matches(\"RateBeer\",SAT_match_candidates,df_rb_beer_wrating)\n",
    "\n",
    "#Example \n",
    "display(automatic_matches_BA.head(3))\n",
    "display(not_matched_BA.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:34:14.496307Z",
     "start_time": "2022-12-22T16:34:14.472874Z"
    }
   },
   "outputs": [],
   "source": [
    "#We display each one of the dataframes containing non matched beers to check entry by entry\n",
    "#display(manual_match_RB[[\"nom\",\"alcool\",\"RB_beer_name\",\"RB_abv\"]])\n",
    "#display(manual_match_BA[[\"nom\",\"alcool\",\"BA_beer_name\",\"BA_abv\"]])\n",
    "RB_manually_matched_list = [15,20,25,31,35,40,70,100,120,125,130,196,215,220,275,280]\n",
    "BA_manually_matched_list = [15,27,30,40,71,95,100,120,130,150,156,195,215,220,225,250,260,275,280] \n",
    "RB_manually_matched = SAT_match_candidates.iloc[RB_manually_matched_list][[\"nom\",\"RB_beer_name\",\"RB_avg\",\"RB_abv\",\"RB_similarity\",\"RB_brewery_name\",\"RB_style\",\"RB_beer_id\"]].drop_duplicates(subset=\"nom\", keep='first', inplace=False, ignore_index=False)\n",
    "BA_manually_matched = SAT_match_candidates.iloc[BA_manually_matched_list][[\"nom\",\"BA_beer_name\",\"BA_avg\",\"BA_abv\",\"BA_similarity\",\"BA_brewery_name\",\"BA_style\",\"BA_beer_id\"]].drop_duplicates(subset=\"nom\", keep='first', inplace=False, ignore_index=False)\n",
    "\n",
    "\n",
    "#This is the final dataframe with all beers found in the datasets. These beers were either automatically or manually matched\n",
    "RB_matches = pd.concat([automatic_matches_RB,RB_manually_matched],axis=\"index\")\n",
    "BA_matches = pd.concat([automatic_matches_BA,BA_manually_matched],axis=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:08:32.364854Z",
     "start_time": "2022-12-21T20:08:27.547470Z"
    }
   },
   "outputs": [],
   "source": [
    "#We created a helper function to automatically prepare features (Alcohol content, dummy variables for beer production location and beer style) .\n",
    "# More information can be found with help(). A copy of the function is available at the end of the notebook.\n",
    "help(SAT_helpers.prepare_features)\n",
    "sat_rb_features,rb_features, RB_SAT_beer_to_predict, RB_training_beers =   SAT_helpers.prepare_features(\"RateBeer\",RB_matches)  \n",
    "sat_ba_features,ba_features, BA_SAT_beer_to_predict, BA_training_beers =  SAT_helpers.prepare_features(\"BeerAdvocate\",BA_matches)  \n",
    "\n",
    "#Example\n",
    "display(sat_rb_features.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:26:28.103622Z",
     "start_time": "2022-12-21T20:08:32.366011Z"
    }
   },
   "outputs": [],
   "source": [
    "#We created a helper function to automatically train a regression model and evaluate it.\n",
    "#Moreover, this function also outputs the predictions for SAT beers that were not found in the dataset (our main goal in this step).\n",
    "# More information can be found with help(). A copy of the function is available at the end of the notebook.\n",
    "\n",
    "help(SAT_helpers.randomforest_sat_beers_ratings)\n",
    "RB_r2_score, RB_predictions = SAT_helpers.randomforest_sat_beers_ratings(rb_features,sat_rb_features)\n",
    "BA_r2_score, BA_predictions = SAT_helpers.randomforest_sat_beers_ratings(ba_features,sat_ba_features)\n",
    "print(f\"The R2 score for ratings estimation with our Random Forest Regressor applied to SAT beers with RateBeer is: {RB_r2_score}\")\n",
    "print(f\"The R2 score for ratings estimation with our Random Forest Regressor applied to SAT beers with RateBeer is: {BA_r2_score}\")\n",
    "\n",
    "#Unfortunately, our results are not spetacular. Somewhat expected since we use very limited features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-22T16:34:31.374624Z",
     "start_time": "2022-12-22T16:34:31.344684Z"
    }
   },
   "outputs": [],
   "source": [
    "#We created a helper function to automatically display and save our results for further use in the notebook.\n",
    "# More information can be found with help(). A copy of the function is available at the end of the notebook.\n",
    "\n",
    "\n",
    "help(SAT_helpers.save_and_display_sat_ratings) \n",
    "RB_sorted = SAT_helpers.save_and_display_sat_ratings(\"RateBeer\",RB_predictions,RB_matches,RB_SAT_beer_to_predict,RB_training_beers)\n",
    "BA_sorted = SAT_helpers.save_and_display_sat_ratings(\"BeerAdvocate\",BA_predictions,BA_matches,BA_SAT_beer_to_predict,BA_training_beers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the dataset for the beers prefered by each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:26:30.904964Z",
     "start_time": "2022-12-21T20:26:30.902575Z"
    }
   },
   "outputs": [],
   "source": [
    "#We created a helper function to automatically search the notebook for the prefered beers of users of each country.\n",
    "# A brief explanation of the steps performed by the function are :\n",
    "#Note : our functions use the 'unbiased_ratings' we generated in our bias correction step.\n",
    "#1. Group all ratings by country of origin of its corresponding user by summing and counting all ratings\n",
    "#2. Calculate the average rating for each beer according to each country of origin \n",
    "#3. Find the most rated beer and the beer that has the highest rating, given that the beer is at least in the 50% quantile of number of ratings (i.e. the beer is not rated by only a few individuals).\n",
    "#(bis): Redo step 1 to 3 in parallel, but considering beer styles instead of separate beers.\n",
    "\n",
    "help(read_data.find_favourite_beers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:28:57.157487Z",
     "start_time": "2022-12-21T20:26:30.906142Z"
    }
   },
   "outputs": [],
   "source": [
    "most_reviewed_beer_RB,favorite_beer_RB,most_reviewed_style_RB,favorite_style_RB = read_data.find_favourite_beers(\"RateBeer\")\n",
    "most_reviewed_beer_BA,favorite_beer_BA,most_reviewed_style_BA,favorite_style_BA = read_data.find_favourite_beers(\"BeerAdvocate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:28:57.181901Z",
     "start_time": "2022-12-21T20:28:57.160693Z"
    }
   },
   "outputs": [],
   "source": [
    "#An example:\n",
    "print(\"Favourite beer style of BeerAdvocate users, by country:\")\n",
    "display(favorite_style_BA)\n",
    "\n",
    "print(\"Favourite beer of RateBeer users, by country:\")\n",
    "display(favorite_beer_RB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs and Plots for Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beer and Country t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:28:57.186952Z",
     "start_time": "2022-12-21T20:28:57.183966Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from functions import plot_helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:29:52.513499Z",
     "start_time": "2022-12-21T20:28:57.188585Z"
    }
   },
   "outputs": [],
   "source": [
    "#We implemented a function to retrieve the reviews of the subset of the SAT beers that are in the datasets\n",
    "help(plot_helpers.retrieve_reviews_SAT)\n",
    "\n",
    "best_RB_reviews, best_reviews_SAT = plot_helpers.retrieve_reviews_SAT(\"RateBeer\")\n",
    "best_BA_reviews, best_reviews_SAT_BA = plot_helpers.retrieve_reviews_SAT(\"BeerAdvocate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:29:52.539082Z",
     "start_time": "2022-12-21T20:29:52.517268Z"
    }
   },
   "outputs": [],
   "source": [
    "#We add country information to the dataset in order to allows us to plot countries separatedly from SAT beers\n",
    "#We consider good reviews all reviews that are not empty (more than one word) and that are associated with high ratings\n",
    "best_RB_reviews = best_RB_reviews[best_RB_reviews[\"good_reviews\"].str.len() > 1]\n",
    "best_BA_reviews = best_BA_reviews[best_BA_reviews[\"good_reviews\"].str.len() > 1]\n",
    "help(plot_helpers.add_country_column)\n",
    "best_BA_reviews_with_countries = plot_helpers.add_country_column(best_BA_reviews,\"DATA/favourite_beer_BA.csv\",california_as_usa=True)\n",
    "best_RB_reviews_with_countries = plot_helpers.add_country_column(best_RB_reviews,\"DATA/favourite_beer_RB.csv\",california_as_usa=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:39:31.567831Z",
     "start_time": "2022-12-21T20:30:44.897268Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transforming reviews to embeddings with OpenAI API and ADA model ##\n",
    "#We delegate the hard job of producing word embeddings to a foundation model. \n",
    "#In this case, ADA-002, launched on 16/12/2022\n",
    "\n",
    "help(plot_helpers.request_embeddings)\n",
    "openai.api_key = \"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\"\n",
    "RB_embeddings = plot_helpers.request_embeddings(best_RB_reviews_with_countries[\"good_reviews\"],verbose=False)\n",
    "SAT_embeddings = plot_helpers.request_embeddings(best_reviews_SAT[\"good_reviews\"],verbose=False)\n",
    "BA_embeddings = plot_helpers.request_embeddings(best_BA_reviews_with_countries[\"good_reviews\"],verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:40:37.689295Z",
     "start_time": "2022-12-21T20:40:34.200623Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plot_helpers.plot_tsne)\n",
    "\n",
    "plot_helpers.plot_tsne(SAT_embeddings,RB_embeddings,best_reviews_SAT,best_RB_reviews_with_countries,perplexity=15,country_size=10,beer_size=25,acronym=\"RB\",title=\"t-SNE representation of best beers sold on SAT and favourite beers of each country according to RateBeer\")\n",
    "plot_helpers.plot_tsne(SAT_embeddings,BA_embeddings,best_reviews_SAT,best_BA_reviews_with_countries,perplexity=15,country_size=20,beer_size=25,acronym=\"BA\",title=\"t-SNE representation of best beers sold on SAT and favourite beers of each country according to BeerAdvocate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Beer rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:45:29.709919Z",
     "start_time": "2022-12-21T20:45:29.594494Z"
    }
   },
   "outputs": [],
   "source": [
    "#We plot the rankings of SAT beers according to both datasets. We provide a button so you can choose the natural ranking\n",
    "\n",
    "RB_sorted = pd.read_csv(\"DATA/predicted_SAT_RB_sorted.csv\")\n",
    "BA_sorted = pd.read_csv(\"DATA/predicted_SAT_BA_sorted.csv\")\n",
    "\n",
    "help(plot_helpers.create_rank_plot)\n",
    "plot_helpers.create_rank_plot(BA_sorted,RB_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:39:35.336361Z",
     "start_time": "2022-12-21T20:39:35.336352Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plot_helpers.plot_wordcloud_dropdown)\n",
    "plot_helpers.plot_wordcloud_dropdown(\"BeerAdvocate\")\n",
    "plot_helpers.plot_wordcloud_dropdown(\"RateBeer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive World Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:39:35.337610Z",
     "start_time": "2022-12-21T20:39:35.337601Z"
    }
   },
   "outputs": [],
   "source": [
    "#We created our own function based on plotly to prepare our interactive World Map. \n",
    "#This function was put in a separated map_helpers.py for clarity purposes\n",
    "# help(map_helpers) to see all we used to produce this plot.\n",
    "from functions import map_generation\n",
    "from functions import plot_helpers\n",
    "\n",
    "help(plot_helpers)\n",
    "plot_helpers.combine_neg_pos_and_favoured_beer('RateBeer_pos_neg_analysis_resume.csv',\n",
    "                                              'favourite_beer_RB.csv',\n",
    "                                             'RB_map_source.csv')\n",
    "\n",
    "plot_helpers.combine_neg_pos_and_favoured_beer('BeerAdv_pos_neg_analysis_resume.csv',\n",
    "                                              'favourite_beer_BA.csv',\n",
    "                                             'BA_map_source.csv')\n",
    "plot_helpers.generate_map('RB_map_source.csv', 'map_favourite_beer_RB',title=\"Favourite beer of users by country according to RateBeer\")\n",
    "plot_helpers.generate_map('BA_map_source.csv', 'map_favourite_beer_BA',title=\"Favourite beer of users by country according to BeerAdvocate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum : Our helper modules exposed for easy reading and code-checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP_utils.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:39:35.338674Z",
     "start_time": "2022-12-21T20:39:35.338662Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "def vector_space_retrieval(queries,dataframe,k=5):\n",
    "    \"\"\"\n",
    "    implementation of Vector Space retrieval with cosine similarities\n",
    "     Parameters\n",
    "    ----------\n",
    "    queries   (pandas.Series)    : dataframe with concatenated tokens used as a 'query' for information retrieval\n",
    "    dataframe (pandas.DataFrame) : corpus considered for retrieval\n",
    "    k          (int)             : number of matches to be found\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (pandas.DataFrame) DataFrame with k top matches for 'queries'. Has a column for the cosine similarity value calculated during retrieval\n",
    "    '''\n",
    "    \"\"\"\n",
    "    document_dict = tokenize_dataframe(dataframe)\n",
    "    doc_vectors = tfidf.fit_transform(document_dict.values())\n",
    "    doc_ids = []\n",
    "    similarity_coefs = []\n",
    "    for query in queries.values():\n",
    "        vector_queries = tfidf.transform([query])\n",
    "        cosine_similarities = linear_kernel(vector_queries, doc_vectors).flatten()\n",
    "        related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                        reverse=True))\n",
    "        for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "            if i >= k:\n",
    "                break\n",
    "            doc_ids.append(related_docs_indices[i])\n",
    "            similarity_coefs.append(cos_sim)\n",
    "    new_df = dataframe.iloc[doc_ids].copy()\n",
    "    new_df[\"similarity\"] = np.array(similarity_coefs)\n",
    "    return new_df\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Transforms a 'text' into a collection of tokens. Implemented with lowercase, stemming and punctuation removal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text         (string)  : text to be tokenizec\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (string) tokenized text\n",
    "    '''\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "def tokenize_dataframe(dataframe):\n",
    "    \"\"\"tokenizes a specific combination (abv, beer_name and brewery_name) of columns of a beer dataset. Used for preparing queries and documents information retrieval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe    (string)  : dataframe with columns to be tokenized and used to form documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list(string)) list of documents \n",
    "    \"\"\"\n",
    "    documents = {}\n",
    "    for index, beer in dataframe.iterrows():\n",
    "        documents[beer[\"beer_id\"]] = (tokenize(beer[\"beer_name\"]) + \" \" + tokenize(beer['brewery_name']) + \" \" + tokenize(str(beer[\"abv\"])))\n",
    "    return documents\n",
    "\n",
    "def search_vec(query, features, threshold=0.1):\n",
    "    new_features = tfidf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids\n",
    "\n",
    "def querify(query_df):\n",
    "    \"\"\" produces queries for the SAT beer dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe    (string)  : dataframe of SAT beers. Must have columns 'nom', 'alcool' and 'brasseur'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list(string)) list of queries\n",
    "    \"\"\"\n",
    "    sat_queries = {}\n",
    "    for index, biere in query_df.iterrows():\n",
    "        sat_queries[index] = (tokenize(biere['nom']) +\" \"  + tokenize(biere['brasseur']) + \" \" + tokenize(str(biere['alcool'])))\n",
    "    return sat_queries\n",
    "\n",
    "def display_results_df(base_df,results_df,reference_column,results):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[reference_column] = base_df[reference_column]\n",
    "    new_df[results] = results_df[results].copy()\n",
    "    return new_df\n",
    "\n",
    "### Helper functions for word count, language identification and date \n",
    "def fetch_ratings(dataset):\n",
    "    \"\"\"fetches ratings of beers for plotting and exploratory data analysis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset    (string)  : name of the dataset. Either 'BeerAdvocate' or 'RateBeer'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (pandas.Series) pandas.Series with all the ratings of the dataset\n",
    "    \"\"\"\n",
    "    first = pd.read_csv(f\"data/{dataset}_ratings_part_0.csv\",low_memory=False)\n",
    "    if dataset == \"BeerAdvocate\" :\n",
    "        first = first[first['rating'] != ' nan']\n",
    "    if dataset == \"RateBeer\" :\n",
    "        first = first[first['rating'] != 'NaN']    \n",
    "    ratings = first.rating.astype(float)\n",
    "    if dataset == \"BeerAdvocate\" :\n",
    "        csv_count = 17\n",
    "    else :\n",
    "        csv_count = 15\n",
    "\n",
    "    for index in range(1,csv_count):   \n",
    "        temp = pd.read_csv(f\"data/{dataset}_ratings_part_{index}.csv\",low_memory=False)\n",
    "        if dataset == \"BeerAdvocate\" :\n",
    "            first = temp[temp['rating'] != 'nan']\n",
    "        if dataset == \"RateBeer\" :\n",
    "            first = temp[temp['rating'] != 'NaN']        \n",
    "        \n",
    "        rating = temp.rating.astype(float)\n",
    "        ratings = pd.concat([ratings, rating])\n",
    "    return ratings\n",
    "\n",
    "def summary_analysis(dataset):\n",
    "    \"\"\"produces a dataframe of word counts by rating and of datetimes of rating creationf for exploratory data analysis purposes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset     (string)  : name of the dataset. Either 'RateBeer' or 'BeerAdvocate\n",
    "    Returns\n",
    "    -------\n",
    "    (series) series of wordcounts for each rating\n",
    "    (series) series of correctly encoded datetime.Datetimes for the dates of creationof reviews\n",
    "    \n",
    "    \"\"\"\n",
    "    first = pd.read_csv(f\"data/{dataset}_ratings_part_0.csv\",low_memory=False)\n",
    "    \n",
    "    #NaNs between datasets are not standardized in the txt file\n",
    "    if dataset == \"BeerAdvocate\" :\n",
    "        first = first[first['rating'] != ' nan']\n",
    "    if dataset == \"RateBeer\" :\n",
    "        first = first[first['rating'] != 'NaN']\n",
    "    print(\"Started counting words and binning dates...\")\n",
    "    first[\"word_count\"] = first.text.apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    #We filter reviews which have only one word (NaNs in BeerAdvocate mostly)\n",
    "    first = first[first['word_count'] > 1]\n",
    "    \n",
    "    counts = first.word_count\n",
    "    dates = first.date\n",
    "    #We iterate over the csvs used to keep the data, in order to not load the full dataset in memory\n",
    "    if dataset == \"BeerAdvocate\" :\n",
    "        csv_count = 17\n",
    "    else :\n",
    "        csv_count = 15\n",
    "    for index in range(1,csv_count):\n",
    "        temp = pd.read_csv(f\"data/{dataset}_ratings_part_{index}.csv\",low_memory=False)\n",
    "        temp[\"word_count\"] = temp.text.apply(lambda x: len(str(x).split()))\n",
    "        #We filter reviews which have only one word (NaNs in BeerAdvocate mostly)\n",
    "        temp = temp[temp['word_count'] > 1]\n",
    "        count = temp.word_count\n",
    "        date = temp.date\n",
    "        counts = pd.concat([counts, count])\n",
    "        dates = pd.concat([dates, date])\n",
    "    print(\"Done\")\n",
    "    return counts, dates\n",
    "\n",
    "\n",
    "\n",
    "def debiasing(website,beer_df,unique_user):\n",
    "    \"\"\"corrects for the bias of a given dataset. The correction heuristic is inspired from https://krisjensen.github.io/files/bias_blog.pdf/.\n",
    "    Correction is implemented with clipping (such that all ratings are between 0 and 5) and attenuation (such that users with only 1 rating are not corrected and that the correction increases with number of ratings)\n",
    "    \n",
    "    Parameters\n",
    "    ----------                                  \n",
    "    website         (string)  :  name of the website/dataset considered. Either \"RateBeer\" or \"BeerAdvocate\"\n",
    "    beer_df         (pandas.DataFrame) : dataframe with beer ratings that will be corrected\n",
    "    unique_user     (pandas.DataFrame) : dataframe of beer reviewers (without duplicates) used as a basis to determine systematic reviewer bias \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def attenuating(row,max_rating):\n",
    "        \"\"\"attenuates the bias correction of a specific user. \n",
    "        Attenuation is an affine function of the number of ratings of a given user.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        row: row of the given dataframe\n",
    "        max_rating: maximum rating found in the dataframe\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        attenuation coefficient \n",
    "        \"\"\"\n",
    "        if row.nbr_ratings==1:\n",
    "            attenuation_coeff=0 #We cancel the bias if the user only rated once.\n",
    "        if row.nbr_ratings==max_rating:\n",
    "            attenuation_coeff=1 #If the user has the rated the most, we do not attenuate their bias.\n",
    "        else:\n",
    "            attenuation_coeff=1/(max_rating-1)*row.nbr_ratings-1/(max_rating-1) #for the other users, the bias is attenuated with a coefficient between 0 and 1 affine function of the number of ratings\n",
    "        return attenuation_coeff\n",
    "    \n",
    "    def clip (dataframe):\n",
    "        \"\"\"clips a debiased rating such that all ratings are in [0,5] range\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe: dataframe on which the ratings will be debiased\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        debiased_rating: debiased rating in [0,5] range\n",
    "        \"\"\"\n",
    "        debiased_rating=dataframe['rating']-dataframe['bias'] #We compute the debiased rating as the bias of the user substracted to the initial rating of the user\n",
    "        if debiased_rating<0: \n",
    "            debiased_rating=0 #if the new rating is inferior to 0, we clip it to 0\n",
    "        if debiased_rating>5: \n",
    "            debiased_rating=5 #if the new rating is superior to 5, we clip it to 5\n",
    "        return debiased_rating\n",
    "    \n",
    "    # We define local variables depending on the website\n",
    "    if website == \"RateBeer\":\n",
    "        acronym = \"RB\"\n",
    "        NUM_CSV = 15\n",
    "        average_rating = beer_df.avg.mean()\n",
    "    if website == \"BeerAdvocate\":\n",
    "        acronym = \"BA\"\n",
    "        NUM_CSV = 17\n",
    "        average_rating = beer_df.avg.mean()\n",
    "    #We iterate over the ratings dataset and group ratings by user.\n",
    "    grouped_by_users = pd.DataFrame([])\n",
    "    for i in range(1,NUM_CSV):\n",
    "        temp = pd.read_csv(f'DATA/{website}_ratings_part_{i}.csv')\n",
    "        df_partial_grouped_by_users_ratings=temp.groupby([\"user_id\"]).rating.sum().to_frame()#We group the ratings by users and return the sum of the ratings of each user \n",
    "        grouped_by_users = pd.concat([grouped_by_users,df_partial_grouped_by_users_ratings]).groupby([\"user_id\"]).sum()# We compile the results from  all our csvs    \n",
    "        del temp\n",
    "        \n",
    "        \n",
    "    grouped_by_users = grouped_by_users.reset_index()\n",
    "    user_to_nbr_ratings=dict(zip(unique_user.user_id,unique_user.nbr_ratings)) #create a dictionary which keys are user_ids and values are the number of ratings of said users\n",
    "    grouped_by_users[\"nbr_ratings\"]=grouped_by_users.user_id.map(user_to_nbr_ratings) #we map the number of ratings to the corresponding user_ids\n",
    "    #We calculate the attenuation coefficient and bias of each user\n",
    "    maximum_rating=max(grouped_by_users[\"nbr_ratings\"])\n",
    "    grouped_by_users[\"attenuation coeff\"]=grouped_by_users.apply(lambda row: attenuating(row,maximum_rating),axis=1)\n",
    "    #Since we computed the sum of ratings of each users when we iterated over the csvs, we need to divide this value by the number of ratings of the user to get the average. The bias is the average rating of the user - average rating of all beers\n",
    "    grouped_by_users[\"bias\"]=(grouped_by_users[\"rating\"]/grouped_by_users[\"nbr_ratings\"]-average_rating)*grouped_by_users[\"attenuation coeff\"] #this bias must be multiplied by the attenuation coefficient\n",
    "    #We apply the correction to each rating of the dataframe and clip the rating so its always between [0,5]\n",
    "    user_to_bias=dict(zip(grouped_by_users.user_id,grouped_by_users.bias)) #We create a dictionary which keys are the user ids and and values are the user biases\n",
    "    grouped_by_beer = pd.DataFrame([])\n",
    "    for i in range(0,NUM_CSV): #We iterate over all the csvs\n",
    "        temp = pd.read_csv(f'DATA/{website}_ratings_part_{i}.csv')\n",
    "        temp[\"bias\"]=temp.user_id.map(user_to_bias) #we map the bias to its user\n",
    "        temp[\"debiased_rating\"]=temp[\"rating\"]-temp[\"bias\"] #We substract the bias from the initial rating to get the debiased rating\n",
    "        temp[\"debiased_rating\"]=temp.apply(clip,axis=1) #Clipping of the new rating\n",
    "        #Needed to save work, only need to be done once\n",
    "        #temp.to_csv(f\"DATA/{website}_ratings_part_{i}_corrected_w_attenuation.csv\")\n",
    "        partial_grouped_by_beer = temp.groupby([\"beer_id\"]).debiased_rating.sum().to_frame() #We group the debiased ratings by beer ids and return the sum of ratings for each beers\n",
    "        grouped_by_beer = pd.concat([grouped_by_beer,partial_grouped_by_beer]).groupby([\"beer_id\"]).sum() #We compile the results from each csv\n",
    "\n",
    "\n",
    "    grouped_by_beer=grouped_by_beer.reset_index()\n",
    "    beer_to_debiased_rating=dict(zip(grouped_by_beer.beer_id,grouped_by_beer.debiased_rating)) #we create a dictionary which keys are the beer ids and values are the sums of debiased ratings\n",
    "    beer_to_nbr_ratings=dict(zip(beer_df.beer_id,beer_df.nbr_ratings)) #we create a dictionary which keys are the beer ids and the keys are the number of ratings for those beers\n",
    "    #We compute the debiased average of all beers as the sum of debiased ratings divided by the number of ratings\n",
    "    beer_df[\"debiased_avg\"]=beer_df.beer_id.map(beer_to_debiased_rating)/beer_df.beer_id.map(beer_to_nbr_ratings) \n",
    "    beer_df.to_csv(f\"DATA/{website}_beers_corrected_avg.csv\",index=False)\n",
    "\n",
    "\n",
    "Anglo_American_Ales=['Altbier', 'Barley Wine',\"Bitter\",'Premium Bitter/ESB',\"Golden Ale/Blond Ale\",\"Brown Ale\", \"California Common\",\"Cream Ale\",\"Black IPA\",\"India Pale Ale (IPA)\",\"Imperial IPA\",\"Session IPA\",\"Kölsch\",\"American Pale Ale\",\"Irish Ale\",\"English Strong Ale\", \"American Strong Ale\",\"Mild Ale\",\"Amber Ale\",\"English Pale Ale\",\"Traditional ALe\",\"Scotch Ale\",\"Old Ale\",\"Scottish Ale\"]\n",
    "Beligan_Style_Ales=[\"Belgian Ale\",\"Belgian Strong Ale\",\"Bière de Garde\",\"Abbey Dubbel\",'Abt/Quadrupel',\"Saison\",\"Abbey Tripel\"]\n",
    "Lagers=[\"Pale Lager\",\"Premium Lager\",\"Imperial Pils/Strong Pale Lager\",\"India Style Lager\",\"Amber Lager/Vienna\",'Czech Pilsner (Světlý)',\"Pilsener\",\"Heller Bock\",\"Doppelbock\",\"Dumbler Bock\",\"Weizen Bock\",\"Esibock\",\"Malt Liquor\",\"Oktoberfest/Märzen\",\"Radler/Shandy\",\"Zwickel/Keller/Landbier\",\"Dortmunder/Helles\",'Dunkel/Tmavý','Schwarzbier','Polotmavý']\n",
    "Stout_and_Porter=[\"Stout\",\"Imperial Stout\",\"Foreign Stout\",\"Sweet Stout\",\"Dry Stout\",\"Porter\",\"Baltic Porter\",\"Imperial Porter\"]\n",
    "Wheat_beer=[\"Wheat Ale\",\"Witbier\",'German Hefeweizen','Dunkelweizen','German Kristallweizen']\n",
    "Sour_beer=[\"Berliner Weisse\",\"Sour/Wild Ale\",\"Sour Red/Brown\",'Grodziskie/Gose/Lichtenhainer','Lambic Style - Gueuze', 'Lambic Style - Unblended','Lambic Style - Faro','Lambic Style - Fruit',\"Grodziskie/Gose/Lichtenhainer\"]\n",
    "Other_styles=[\"Spice/Herb/Vegetable\",\"Smoked\",'Fruit Beer',\"Sahti/Gotlandsdricke/Koduõlu\",'Low Alcohol','Specialty Grain']\n",
    "Cider_Mead_Saké=['Cider','Mead','Saké - Daiginjo', 'Saké - Namasaké','Saké - Ginjo', 'Saké - Infused', 'Saké - Tokubetsu','Saké - Junmai', 'Saké - Nigori', 'Saké - Koshu', 'Saké - Taru','Saké - Honjozo', 'Saké - Genshu', 'Saké - Futsu-shu','Perry']\n",
    "beer_style_dict={key: \"Anglo American Ales\" for key in Anglo_American_Ales}|{key: \"Belgian Style Ales\" for key in Beligan_Style_Ales}|{key:\"Lagers\" for key in Lagers}|{key:\"Stout and Porter\" for key in Stout_and_Porter}|{key:\"Wheat beer\" for key in Wheat_beer}|{key:\"Sour beer\" for key in Sour_beer}|{key:\"Other styles\" for key in Other_styles}|{key:\"Cider, Mead and Saké\" for key in Cider_Mead_Saké}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T20:44:30.071130Z",
     "start_time": "2022-12-21T20:44:29.995190Z"
    }
   },
   "outputs": [],
   "source": [
    "STATES = {\n",
    "\"Alabama\": \"AL\",\n",
    "\"Alaska\": \"AK\",\n",
    "\"Arizona\": \"AZ\",\n",
    "\"Arkansas\": \"AR\",\n",
    "\"California\": \"CA\",\n",
    "\"Colorado\": \"CO\",\n",
    "\"Connecticut\": \"CT\",\n",
    "\"Delaware\": \"DE\",\n",
    "\"Florida\": \"FL\",\n",
    "\"Georgia\": \"GA\",\n",
    "\"Hawaii\": \"HI\",\n",
    "\"Idaho\": \"ID\",\n",
    "\"Illinois\": \"IL\",\n",
    "\"Indiana\": \"IN\",\n",
    "\"Iowa\": \"IA\",\n",
    "\"Kansas\": \"KS\",\n",
    "\"Kentucky\": \"KY\",\n",
    "\"Louisiana\": \"LA\",\n",
    "\"Maine\": \"ME\",\n",
    "\"Maryland\": \"MD\",\n",
    "\"Massachusetts\": \"MA\",\n",
    "\"Michigan\": \"MI\",\n",
    "\"Minnesota\": \"MN\",\n",
    "\"Mississippi\": \"MS\",\n",
    "\"Missouri\": \"MO\",\n",
    "\"Montana\": \"MT\",\n",
    "\"Nebraska\": \"NE\",\n",
    "\"Nevada\": \"NV\",\n",
    "\"New Hampshire\": \"NH\",\n",
    "\"New Jersey\": \"NJ\",\n",
    "\"New Mexico\": \"NM\",\n",
    "\"New York\": \"NY\",\n",
    "\"North Carolina\": \"NC\",\n",
    "\"North Dakota\": \"ND\",\n",
    "\"Ohio\": \"OH\",\n",
    "\"Oklahoma\": \"OK\",\n",
    "\"Oregon\": \"OR\",\n",
    "\"Pennsylvania\": \"PA\",\n",
    "\"Rhode Island\": \"RI\",\n",
    "\"South Carolina\": \"SC\",\n",
    "\"South Dakota\": \"SD\",\n",
    "\"Tennessee\": \"TN\",\n",
    "\"Texas\": \"TX\",\n",
    "\"Utah\": \"UT\",\n",
    "\"Vermont\": \"VT\",\n",
    "\"Virginia\": \"VA\",\n",
    "\"Washington\": \"WA\",\n",
    "\"West Virginia\": \"WV\",\n",
    "\"Wisconsin\": \"WI\",\n",
    "\"Wyoming\": \"WY\",\n",
    "\"District of Columbia\": \"DC\",\n",
    "\"American Samoa\": \"AS\",\n",
    "\"Guam\": \"GU\",\n",
    "\"Northern Mariana Islands\": \"MP\",\n",
    "\"Puerto Rico\": \"PR\",\n",
    "\"United States Minor Outlying Islands\": \"UM\",\n",
    "\"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "\n",
    "def generate_map(filename, map_name, usa= False, \n",
    "                 html = True, show_map = True,\n",
    "                 title = '',\n",
    "                 source_file_path = 'map/file_for_map/', \n",
    "                 html_file_path = 'map/html/'):\n",
    "\n",
    "    ''' Author: Gabriel Benato @HOTCHOCOLATE, ADA2022\n",
    "    This function generate an interactive map of the world (and USA)\n",
    "    with the average score of beer per countries and their favored beer's style. \n",
    "    Is is assumed it will be used in a file at the root of the project. \n",
    "    If it's not the case see: source_file_path and html_file_path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename         (string)  :  Name of the source file containing the necessary dataframe for map generation \n",
    "                                  should contain a 'location', 'style', 'normalized_rating', 'pos_words' and 'neg_words' column.\n",
    "    map_name         (string)  :  HTML file's name for the resulting interactive map.\n",
    "    usa              (boolean) :  Activate the generation of the USA's map.\n",
    "    html             (boolean) :  Activate the generation of html file.\n",
    "    show_map         (boolean) :  Show the produced map(s).\n",
    "    title            (string)  :  Title of the plot.\n",
    "    source_file_path (string)  :  Path to the source file.\n",
    "    html_file_path   (string)  :  Path to the futur html file.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None, but may generate html file of interactive map and show generated map.\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(source_file_path + filename)\n",
    "    \n",
    "    ### NOTE: Clean corrrupted data but we should do it before \n",
    "    for i, e in enumerate(data['location']):\n",
    "        if \"http\" in e or \"<\" in e:\n",
    "            data = data.drop(i)\n",
    "    data.reset_index(inplace = True, drop = True) #reset index so we don't make error due to assumption of continuous index\n",
    "\n",
    "    # We have multiple occurence of the USA (multiple states) \n",
    "    # but we will only keep the best one for the world map \n",
    "    # We will also set-up a way to have a look only in the United Sates \n",
    "    location_country = data.copy()\n",
    "    mask = [False] * data.shape[0]\n",
    "    \n",
    "    for j, country in enumerate(data['location']):\n",
    "        if \"United States\" in country:\n",
    "            mask[j] = True #Prepare mask for united states only dataframe (united_states)\n",
    "            if \"California\" in country:\n",
    "                #Delete the State\n",
    "                location_country['location'][j] = \"United States\"\n",
    "            else:\n",
    "                #get rid of all the occurance of United States except Florida since it is the most populated state\n",
    "                location_country = location_country.drop(j) \n",
    "                \n",
    "    hover_data_world = np.stack((location_country[\"beer_name\"],\n",
    "                                 location_country[\"brewery_name\"],\n",
    "                                 location_country[\"normalized_rating\"],\n",
    "                                 location_country[\"style\"],\n",
    "                                 location_country[\"pos_words\"],\n",
    "                                 location_country[\"neg_words\"]), axis=-1)\n",
    "\n",
    "    #Plot the worldwide figure\n",
    "    fig_world = go.Figure(data = go.Choropleth(\n",
    "        locations = location_country['location'], #counties's nams are used to place data on the world map\n",
    "        locationmode= 'country names',\n",
    "        z = location_country['normalized_rating'], #data that describes the choropleth value-to-color mapping\n",
    "        text = location_country['location'], #pop-up for each country \n",
    "        colorscale = 'Viridis',\n",
    "        autocolorscale=False,\n",
    "        reversescale=True,\n",
    "        marker_line_color='darkgray',\n",
    "        marker_line_width=0.5,\n",
    "        colorbar_tickprefix = 'average rating ',\n",
    "        colorbar_title = \"Mean average rating\",\n",
    "        customdata = hover_data_world,\n",
    "        hovertemplate=\"\"\"   <br><b>Country</b>: %{text}\n",
    "                            <br><b>Beer</b>: %{customdata[0]}\n",
    "                            <br><b>Brewery</b>: %{customdata[1]}\n",
    "                            <br><b>Mean Rating</b>: %{customdata[2]:.2f}\n",
    "                            <br><b>Type</b>: %{customdata[3]}\n",
    "                            <br><b>Positive words in reviews</b>: %{customdata[4]}\n",
    "                            <br><b>Positive words in reviews</b>: %{customdata[5]}<br><extra></extra>\"\"\"\n",
    "    ))\n",
    "\n",
    "    fig_world.update_layout(\n",
    "        title_text=title,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)', \n",
    "    )\n",
    "    #print the worldwide map\n",
    "    if show_map :\n",
    "        fig_world.show()\n",
    "    #Create an html file of the map for the site \n",
    "    if html : \n",
    "        fig_world.write_html(html_file_path + map_name +\"_country.html\")\n",
    "        \n",
    "    #Activate if we want USA map\n",
    "    if usa:\n",
    "        united_states = data[mask]\n",
    "        for k, state in enumerate(united_states['location']):\n",
    "            #Only keep the states \n",
    "            united_states['location'][k] = state.split('States, ',1)[1]\n",
    "            \n",
    "        #switch state by their abbreviation to fit \n",
    "        #the locationmode 'USA-states' of the plotly library\n",
    "        united_states.location = united_states.location.map(STATES)\n",
    "        hover_data_usa = np.stack((united_states[\"beer_name\"],\n",
    "                                 united_states[\"brewery_name\"],\n",
    "                                 united_states[\"normalized_rating\"],\n",
    "                                 united_states[\"style\"],\n",
    "                                 united_states[\"pos_words\"],\n",
    "                                 united_states[\"neg_words\"]), axis=-1)\n",
    "        \n",
    "        #plot the usa map\n",
    "        fig_usa = go.Figure(data = go.Choropleth(\n",
    "            locations = united_states['location'], #abbreviation are used to place data on the USA map\n",
    "            locationmode= 'USA-states',\n",
    "            z = united_states['normalized_rating'], #data that describes the choropleth value-to-color mapping\n",
    "            text = location_country['location'], #pop-up for each country \n",
    "            colorscale = 'Viridis',\n",
    "            autocolorscale=False,\n",
    "            reversescale=True,\n",
    "            marker_line_color='darkgray',\n",
    "            marker_line_width=0.5,\n",
    "            colorbar_tickprefix = 'average rating ',\n",
    "            colorbar_title = 'Mean average rating',\n",
    "            customdata = hover_data_usa,\n",
    "            hovertemplate=\"\"\"   <br><b>States</b>: %{text}\n",
    "                            <br><b>Beer</b>: %{customdata[0]}\n",
    "                            <br><b>Brewery</b>: %{customdata[1]}\n",
    "                            <br><b>Mean Rating</b>: %{customdata[2]:.2f}\n",
    "                            <br><b>Type</b>: %{customdata[3]}\n",
    "                            <br><b>Positive words in reviews</b>: %{customdata[4]}\n",
    "                            <br><b>Positive words in reviews</b>: %{customdata[5]}<br><extra></extra>\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "        ))\n",
    "\n",
    "        fig_usa.update_layout(\n",
    "            title_text='Zoom on the USA',\n",
    "            geo=dict( scope='usa'), #switch from world-map to USA\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            plot_bgcolor='rgba(0,0,0,0)', \n",
    "        )\n",
    "        #print the map\n",
    "        if show_map :\n",
    "            fig_usa.show()\n",
    "        #generate html file of USA map\n",
    "        if html :\n",
    "            fig_usa.write_html(html_file_path + map_name +\"_usa.html\")\n",
    "    return\n",
    "\n",
    "def combine_neg_pos_and_favoured_beer(neg_pos_filename, favoured_filename, combined_filename, \n",
    "                                       source_file_path = 'map/file_for_map/'):\n",
    "    ''' \n",
    "    Function combines dataset from positive-negative word analysis and favoured beer analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------    \n",
    "    neg_pos_filename  (string)  :  Name of the source file from the favoured beer analysis.\n",
    "    favoured_filename (string)  :  Name of the source file from the favoured beer analysis.\n",
    "    combined_filename (string)  :  Name of the resulting combined file.\n",
    "    source_file_path  (string)  :  Path to the source files\n",
    "    '''\n",
    "    neg_pos = pd.read_csv(source_file_path + neg_pos_filename)\n",
    "    favoured_beer = pd.read_csv(source_file_path + favoured_filename)\n",
    "\n",
    "\n",
    "    merged_data = favoured_beer.merge(neg_pos[[\"location\",\"neg_words\",\"pos_words\"]],\n",
    "                                               how=\"outer\",left_on=\"location\",right_on=\"location\")\n",
    "\n",
    "    merged_data['pos_words'] = merged_data['pos_words'].fillna('Unknown')\n",
    "    merged_data['neg_words'] = merged_data['neg_words'].fillna('Unknown')\n",
    "    merged_data.to_csv(source_file_path + combined_filename)\n",
    "    return\n",
    "\n",
    "def find_high_rating_review(df):\n",
    "    \"\"\" To be used as an aggregation function of GroupBy object (e.g.  pandas.DataFrame.groupby(...).agg(find_high_rating_review))\n",
    "    Filters the input dataframe by selecting rows with reviews that verify:\n",
    "    1. Max rating was given\n",
    "    2. The overall rating (sum of taste, aroma, ...) is not 5 points less than the maximum value\n",
    "    3. The review is not empty (has at least 2 characters)\n",
    "\n",
    "    Arguments\n",
    "    ------\n",
    "    df (pandas.DataFrame) : dataframe in which reviews will be searched\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    (pandas.DataFrame)         : reduced dataframe with only high rating reviews\n",
    "\n",
    "    \"\"\"\n",
    "    max_rating = df[\"rating\"].max()\n",
    "    max_overall = df[\"overall\"].max()\n",
    "    nice_reviews = df[(df[\"rating\"] == max_rating) & (df[\"overall\"] >  max_overall - 5) & (len(df[\"text\"]) > 1)][\"text\"]\n",
    "    nice_reviews_sample = nice_reviews.sample(n=min(5,len(nice_reviews)))\n",
    "    concatenated_nice_reviews = nice_reviews_sample.str.cat(sep=' ')\n",
    "    return pd.DataFrame.from_dict([{\"beer_id\" : df[\"beer_id\"].iloc[0], \"beer_name\" : df[\"beer_name\"].iloc[0], \"style\": df[\"style\"].iloc[0],\"brewery_name\": df[\"brewery_name\"].iloc[0], \"good_reviews\" : concatenated_nice_reviews}])\n",
    "\n",
    "\n",
    "def retrieve_reviews_SAT(website):\n",
    "    \"\"\" Retrieves reviews corresponding to beers sold on SAT in the dataset corresponding to a given 'website' and to beers favoured by each country\n",
    "    Parameters\n",
    "    ----------\n",
    "    website     (string)  : name of the dataset. Either 'RateBeer' or 'BeerAdvocate\n",
    "    Returns\n",
    "    -------\n",
    "    (series) best reviews of the beers favoured by each country\n",
    "    (series) best reviews of the beers sold at SAT that are found in 'website'\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if website == \"RateBeer\":\n",
    "        TOTAL_CSV = 72\n",
    "        acronym = \"RB\"\n",
    "        fav_beer = pd.read_csv(\"DATA/favourite_beer_RB.csv\")[\"beer_id\"].unique().tolist()\n",
    "    if website == \"BeerAdvocate\":\n",
    "        TOTAL_CSV = 26\n",
    "        acronym = \"BA\"\n",
    "        fav_beer = pd.read_csv(\"DATA/favourite_beer_BA.csv\")[\"beer_id\"].unique().tolist()\n",
    "\n",
    "    #Retrieve favourite beer of each country according to users for RB\n",
    "    temp = pd.read_csv(f\"DATA/{website}_reviews_part_0.csv\")\n",
    "    best_reviews = temp[temp[\"beer_id\"].isin(fav_beer)]\n",
    "    \n",
    "    ##Retrieve reviews for SAT beers\n",
    "    SAT_RB_best_beers = pd.read_csv(f\"DATA/predicted_SAT_{acronym}_sorted.csv\",index_col=0)\n",
    "    #Take 10 best SAT beers according to users\n",
    "    bestSAT_RB = SAT_RB_best_beers[f\"{acronym}_beer_id\"].values[0:13]\n",
    "    best_reviews_SAT = temp[temp[\"beer_id\"].isin(bestSAT_RB)]\n",
    "    for i in range(1,TOTAL_CSV):\n",
    "        #Iterate over all the partitioned dataset and populate a dataframe only with the reviews of SAT beers\n",
    "        temp = pd.read_csv(f\"DATA/{website}_reviews_part_{i}.csv\")\n",
    "        best_reviews = pd.concat([best_reviews,temp[temp[\"beer_id\"].isin(fav_beer)]],join=\"outer\")\n",
    "        best_reviews_SAT = pd.concat([best_reviews_SAT,temp[temp[\"beer_id\"].isin(bestSAT_RB)]],join=\"outer\")\n",
    "    best_reviews = best_reviews.groupby(by=\"beer_id\").apply(find_high_rating_review).reset_index(drop=True)\n",
    "    best_reviews_SAT = best_reviews_SAT.groupby(by=\"beer_id\").apply(find_high_rating_review).reset_index(drop=True)\n",
    "    return best_reviews, best_reviews_SAT\n",
    "\n",
    "\n",
    "def add_country_column(target_df,country_csv_path,california_as_usa=True):\n",
    "    \"\"\"\n",
    "    Given a dataset of beer reviews 'target_df', augments the dataset with the location of the reviewer from an accessory dataset located in 'country_csv_path'\n",
    "    \n",
    "    Arguments\n",
    "    ------\n",
    "    target_df           (pandas.DataFrame): dataframe in which the country column will be added\n",
    "    country_csv_path    (string)          : path to file in which csv with country data is located\n",
    "    california_as_usa   (boolean)         : if True, we drop all information about individual USA states and keep only California, which is the most populated one as of 2022\n",
    "    Returns\n",
    "    ------\n",
    "    (pandas.DataFrame) DataFrame with country column added\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Recover the dataframe of favourite beer for users of each country. \n",
    "    #Drop countries for which there were no enough reviewers (tagged as favourite beer_id = -1)\n",
    "    countries = pd.read_csv(country_csv_path)\n",
    "    countries = countries[~ (countries[\"beer_id\"] == -1.)]\n",
    "    best_with_countries = target_df.merge(countries[[\"beer_id\",\"location\"]],how=\"left\",on=\"beer_id\")\n",
    "    #Clean individual states of United States, by keeping only California\n",
    "    if california_as_usa:\n",
    "        mask = (best_with_countries[\"location\"].str.contains(\"United States\")) & ~(best_with_countries[\"location\"].str.contains(\"California\"))\n",
    "        best_with_countries = best_with_countries[~mask]\n",
    "    return best_with_countries\n",
    "\n",
    "\n",
    "def request_embeddings(series,verbose=True):\n",
    "    \"\"\"\n",
    "    Given a series of textual reviews, sends a series of calls to OpenAI Embeddings API for the ADA model embeddings. \n",
    "    Calls are sent on 6 seconds interval to avoid RateLimitError from the OpenAI API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series     (pd.Series)  : Series of reviews to be embedded by ADA-002\n",
    "    verbose    (bool)       : if True, reviews are printed as they are sent to the OpenAI API\n",
    "    Returns\n",
    "    -------\n",
    "    (numpy.array) (len(series),1956) length numpy array corresponding to OpenAi ADA embeddings for each review sent\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    import time\n",
    "    from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "    #We use exponential backoff to limit adaptively our request rate\n",
    "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "    def get_embedding_exponential_bo(text: str, engine=\"text-embedding-ada-002\") -> list[float]:\n",
    "\n",
    "        # replace newlines, which can negatively affect performance.\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "    corpus = series.values\n",
    "    embeddings = []\n",
    "    #We send request to the API on a loop in order to control the request rate and avoid being limited\n",
    "    for (i,x) in enumerate(corpus):\n",
    "        if verbose:\n",
    "            print(\"Sending API request to embed the following review:\\n\")\n",
    "            print(x)\n",
    "        embeddings.append(get_embedding_exponential_bo(x, engine='text-embedding-ada-002'))\n",
    "        #Toggle the waiting time between requests, max request rate is 20/min\n",
    "        time.sleep(6)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    if verbose:\n",
    "        print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "### Plotting a t-SNE with plotly with custom markers ###\n",
    "\n",
    "def plot_tsne(SAT_embeddings,dataset_embeddings,sat_df,dataset_df,perplexity=10,country_size=20,beer_size=20,acronym=\"\",title=\"\"):\n",
    "    \"\"\"\n",
    "    Given arrays of embeddings corresponding to reviews of SAT and of the preferred beers of each country for a given dataset,\n",
    "    plot a t-SNE graph where embeddings corresponding to SAT beers are rendered as beer images and where favourite beers are rendered as the flag of the country that prefers them.\n",
    "    The plot can be customized for perplexity and marker size.\n",
    "\n",
    "    Arguments\n",
    "    --------\n",
    "    SAT_embeddings      (numpy.array)      : (len(sat_df),1916) array corresponding to OpenAI   \n",
    "    dataset_embeddings  (numpy.array)      : (len(dataset_df),1916) array corresponding to OpenAI   \n",
    "    sat_df              (pandas.DataFrame) : dataset from which the reviews corresponding to SAT beers were taken (for rendering data on hover)\n",
    "    dataset_df          (pandas.DataFrame) : dataset from which the reviews corresponding to countries were taken (for rendering data on hover)\n",
    "    perplexity          (int)    : hyperparameter of t-SNE plots\n",
    "    country_size        (int)    : size of country flag images in plot (in pts)\n",
    "    beer_size           (int)    : size of beer figures images in plot (in pts)\n",
    "    acronym             (string) : Prefix used for saving the plot\n",
    "    title               (string) : Title of the plot\n",
    "    \"\"\"\n",
    "    #Beers sold on SAT should be shown with their image, while embeddings representing favoured beers for each\n",
    "    #country should be shown with the country flag.\n",
    "    tsne_vectors = np.concatenate([SAT_embeddings,dataset_embeddings])\n",
    "    sat_label = np.ones(len(SAT_embeddings))\n",
    "    non_sat_label = np.zeros(len(dataset_embeddings))\n",
    "    label = np.concatenate([sat_label,non_sat_label])\n",
    "    SAT_ids = sat_df[\"beer_id\"].values\n",
    "    df = pd.DataFrame()\n",
    "    hover_data = np.stack((pd.concat([sat_df[\"beer_name\"],dataset_df[\"beer_name\"]]),pd.concat([sat_df[\"brewery_name\"],dataset_df[\"brewery_name\"]]),pd.concat([sat_df[\"style\"],dataset_df[\"style\"]])),axis=-1)\n",
    "    df[\"is_sat_beer\"] = label\n",
    "    tsne = TSNE(\n",
    "        n_components=2, perplexity=perplexity, random_state=42, init=\"random\", learning_rate=200,n_iter=10000\n",
    "    )\n",
    "\n",
    "    sat_beer_id = [str(number) for number in SAT_ids]\n",
    "    vis_dims2 = tsne.fit_transform(tsne_vectors)\n",
    "    df[\"x\"] = vis_dims2[:,0]\n",
    "    df[\"y\"] = vis_dims2[:,1]\n",
    "    df[\"general_id\"] = sat_beer_id + list(dataset_df[\"location\"].values)\n",
    "    df[\"beer\"] = list(sat_df[\"beer_name\"].values) + list(dataset_df[\"beer_name\"].values)\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        hover_name=\"general_id\",\n",
    "        hover_data=[\"beer\"],\n",
    "        labels=dict(x=\"t-SNE  first dimension\", y=\"t-SNE second dimension\")\n",
    "\n",
    "    )\n",
    "    fig.update_traces(marker_color=\"rgba(0,0,0,0)\",mode='markers',\n",
    "                      customdata=hover_data,  \n",
    "                      hovertemplate=\"\"\"\n",
    "                            <br><b>Beer</b>: %{customdata[0]}\n",
    "                            <br><b>Brewery</b>: %{customdata[1]}\n",
    "                            <br><b>Style</b>: %{customdata[2]}<br><extra></extra>\"\"\")\n",
    "    maxDim = df[[\"x\", \"y\"]].max().idxmax()\n",
    "    maxi = df[maxDim].max()\n",
    "    for i, row in df.iterrows():\n",
    "        general_id = row[\"general_id\"].replace(\" \",\"-\")\n",
    "        if row.is_sat_beer:\n",
    "            fig.add_layout_image(\n",
    "                dict(\n",
    "                    source=Image.open(f\"Images/beers/{general_id}.0.png\"),\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    xanchor=\"center\",\n",
    "                    yanchor=\"middle\",\n",
    "                    x=row[\"x\"],\n",
    "                    y=row[\"y\"],\n",
    "                    sizex=beer_size,\n",
    "                    sizey=beer_size,\n",
    "                    sizing=\"contain\",\n",
    "                    opacity=1,\n",
    "                    layer=\"above\"\n",
    "                )\n",
    "            )\n",
    "        #We do not have flag images for certain countries ::sad_face::\n",
    "        elif general_id not in[\"Afghanistan\",\"Albania\",\"Bahrain\",\"Benin\",\"Bosnia-and-Herzegovina\",\"Botswana\",\"Bulgaria\",\"Burkina-Faso\",\"Burundi\",\"Cambodia\",\"Central-African-Republic\",\"Comoros\",\"Congo,-Dem.-Rep.\"\n",
    "                          ,\"Congo,-Rep.\",\"Costa-Rica\",\"Cote-d'Ivoire\",\"El-Salvador\",\"Equatorial-Guinea\",\"Eritrea\",\"Ethiopia\",\"Gabon\",\"Gambia\",\"Ghana\",\"Guatemala\",\"Guinea\",\"Guinea-Bissau\",\"Haiti\",\"Honduras\",\"Hong-Kong,-China\",\"Hungary\",\"Indonesia\",\n",
    "                         \"Iraq\",\"Jordan\",\"Kenya\",\"Korea,-Dem.-Rep.\",\"Korea,-Rep.\",\"Kuwait\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Libya\",\"Madagascar\",\n",
    "                         \"Malawi\",\"Malaysia\",\"Mali\",\"Mauritania\",\"Mauritius\",\"Mongolia\",\"Montenegro\",\"Mozambique\",\"Myanmar\",\"Namibia\",\"Nepal\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"Oman\",\"Pakistan\",\"Panama\",\"Paraguay\",\n",
    "                         \"Philippines\",\"Puerto-Rico\",\"Reunion\",\"Romania\",\"Rwanda\",\"Sao-Tome-and-Principe\",\"Saudi-Arabia\",\"Senegal\",\"Serbia\",\"Sierra-Leone\",\"Singapore\",\"Slovak-Republic\",\"Slovenia\",\"Somalia\",\"Sri-Lanka\",\n",
    "                         \"Sudan\",\"Swaziland\",\"Syria\",\"Taiwan\",\"Tanzania\",\"Togo\",\"Trinidad-and-Tobago\",\"Tunisia\",\"Uganda\",\"United-Kingdom\",\"Vietnam\",\"West-Bank-and-Gaza\",\"Yemen,-Rep.\",\"Zambia\",\"Zimbabwe\"]:\n",
    "            fig.add_layout_image(\n",
    "                dict(\n",
    "                    source=Image.open(f\"Images/country_flags/icons8-{general_id.lower()}-100.png\"),\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    xanchor=\"center\",\n",
    "                    yanchor=\"middle\",\n",
    "                    x=row[\"x\"],\n",
    "                    y=row[\"y\"],\n",
    "                    sizex=country_size,\n",
    "                    sizey=country_size,\n",
    "                    sizing=\"contain\",\n",
    "                    opacity=1,\n",
    "                    layer=\"above\"\n",
    "                )\n",
    "            )\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True,showgrid=False,zeroline=False)\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True,showgrid=False,zeroline=False)                        , \n",
    "    fig.update_layout(height=600, \n",
    "                      width=1000, \n",
    "                      paper_bgcolor='rgba(0,0,0,0)',\n",
    "                      plot_bgcolor='rgba(0,0,0,0)',\n",
    "                      title_text=title)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_html(f\"Images/{acronym}_tsne.html\",config = {'displayModeBar': True})\n",
    "def retrieve_style_list(website):\n",
    "    \"\"\"\n",
    "    Returns a list of all beer styles of the dataset of given 'website'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    website         (string)  :  website corresponding to the dataset being processed. Either 'RateBeer' or 'BeerAdvocate'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list) : A lexicographically sorted list of beer styles\n",
    "    \"\"\"\n",
    "    if website == \"RateBeer\":\n",
    "        TOTAL_CSV = 72\n",
    "    if website == \"BeerAdvocate\":\n",
    "        TOTAL_CSV = 26        \n",
    "\n",
    "    style_set = set()\n",
    "    for index in range(0,TOTAL_CSV):\n",
    "            temp = pd.read_csv(f\"DATA/{website}_reviews_part_{index}.csv\",usecols=[\"text\",\"style\"],low_memory=False).astype(str)\n",
    "            styles = set(temp[\"style\"].unique())\n",
    "            style_set.update(styles)\n",
    "    style_list = list(style_set)\n",
    "    style_list.sort()\n",
    "    return style_list\n",
    "def plot_wordcloud_dropdown():\n",
    "    \"\"\"\n",
    "    Plots wordclouds corresponding to beer reviews for all beer styles in RateBeer and BeerAdvocate. Style can be chosen with a dropdown menu\n",
    "    \n",
    "    Arguments\n",
    "    -------\n",
    "    (None)\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    (None) but saves figure in a separate file\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Load images\n",
    "    img_list = os.listdir(\"Images/word_clouds\")\n",
    "    # Initialize figures\n",
    "    fig = go.Figure(layout=go.Layout(width=500, height=500,\n",
    "                                    xaxis=dict(range=[280, 680],\n",
    "                                            fixedrange = False),\n",
    "                                    yaxis=dict(range=[620, 100],\n",
    "                                            fixedrange = False\n",
    "                                    ),\n",
    "                                    ))\n",
    "    #List all styles that will be shown\n",
    "    style_list_BA = retrieve_style_list(\"BeerAdvocate\")\n",
    "    style_list_RB = retrieve_style_list(\"RateBeer\")\n",
    "    style_list = style_list_BA + style_list_RB\n",
    "    description = [\"BeerAdvocate style : \" + style for style in style_list_BA] + [\"RateBeer style : \"+ style for style in style_list]\n",
    "    #Create all renderings in the plot\n",
    "    for i,style in enumerate(style_list):\n",
    "        if \"/\" in style:\n",
    "            style = style.replace(\"/\",\"\")\n",
    "        if i < len(style_list_BA):\n",
    "            pil_img = Image.open(f'Images/word_clouds/BA_{style}_wordcloud.png') # PIL image object\n",
    "        else:\n",
    "            pil_img = Image.open(f'Images/word_clouds/RB_{style}_wordcloud.png') # PIL image object\n",
    "        prefix = \"data:image/png;base64,\"\n",
    "        with BytesIO() as stream:\n",
    "            pil_img.save(stream, format=\"png\")\n",
    "            base64_string = prefix + base64.b64encode(stream.getvalue()).decode(\"utf-8\")\n",
    "        if i == 0:\n",
    "            goImg = go.Image(source=base64_string,\n",
    "                            x0=0, \n",
    "                            y0=0,\n",
    "                            dx=1,\n",
    "                            dy=1,\n",
    "                            visible = True,)\n",
    "        else:\n",
    "            goImg = go.Image(source=base64_string,\n",
    "                        x0=0, \n",
    "                        y0=0,\n",
    "                        dx=1,\n",
    "                        dy=1,\n",
    "                        visible = False,)\n",
    "        fig.add_trace(goImg)\n",
    "        fig.update_traces(\n",
    "                    hovertemplate = None,\n",
    "                    hoverinfo = \"skip\")\n",
    "    #Create masks to activate only one rendering at a time\n",
    "    mask_list = []\n",
    "    mask = np.arange(0,len(style_list))\n",
    "    for i in range(len(style_list)):\n",
    "        mask_list.append(list(mask==i))\n",
    "    buttons = [{'label': description[i], 'method':'update','args':[{\"visible\":mask_list[i]}]} for i,style in enumerate(style_list)]\n",
    "    # Add Annotations and Buttons\n",
    "\n",
    "    fig.update_layout(template=\"simple_white\",\n",
    "                updatemenus=[dict(\n",
    "                active=1,\n",
    "                x=1.05,\n",
    "                y=1.1,\n",
    "                buttons=buttons,\n",
    "            )\n",
    "        ])\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    fig.show()\n",
    "    #Export for site\n",
    "    fig.write_html(f\"Images/test_wordclouds.html\",config = {'displayModeBar': False})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_rank_plot(BA_sorted,RB_sorted):\n",
    "    \"\"\"Creates plots of rankings (with option for normalized rankings) for BeerAdvocate and RateBeer datasets\n",
    "    \n",
    "    Arguments\n",
    "    --------\n",
    "    BA_sorted   (pandas.DataFrame) : dataframe with SAT beers with ratings estimated from/found in BeerAdvocate\n",
    "    RB_sorted   (pandas.DataFrame) : dataframe with SAT beers with ratings estimated from/found in RateBeer\n",
    "    \n",
    "    Returns\n",
    "    ----\n",
    "    (None) but save figure in a separate file\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def compute_normalized_ranking(dataframe):\n",
    "        dataframe[\"prix\"] =  dataframe.apply(lambda row: re.findall(r\"\\d+\\.\\d+\",row[\"prix\"])[-1],axis=1)\n",
    "        dataframe[\"normalized_rating\"] = (dataframe[\"avg\"]/(dataframe[\"vol\"].astype(float)*dataframe[\"prix\"].astype(float)))\n",
    "        dataframe[\"normalized_rating\"] = dataframe[\"normalized_rating\"]\n",
    "        dataframe_normalized_sorted = dataframe.sort_values(by=\"normalized_rating\",ascending=False)\n",
    "        dataframe_normalized_sorted[\"normalized_rating\"] = dataframe_normalized_sorted[\"normalized_rating\"].apply(lambda x : x*5/dataframe_normalized_sorted[\"normalized_rating\"].max())\n",
    "        return dataframe_normalized_sorted\n",
    "    RB_normalized = compute_normalized_ranking(RB_sorted)\n",
    "    BA_normalized = compute_normalized_ranking(BA_sorted)\n",
    "    fig_rank = make_subplots(rows=1, cols=2, horizontal_spacing = 0.1)\n",
    "    customdata_BA  = np.stack((BA_sorted['nom'],BA_sorted['brasseur'],BA_sorted['avg'], BA_sorted['type'],BA_sorted['prix']), axis=-1)\n",
    "    customdata_RB  = np.stack((RB_sorted['nom'],RB_sorted['brasseur'],RB_sorted['avg'], RB_sorted['type'],RB_sorted['prix']), axis=-1)\n",
    "    customdata_normalized_BA  = np.stack((BA_normalized['nom'],BA_normalized['brasseur'],BA_normalized['normalized_rating'], BA_normalized['type'],BA_normalized['prix']), axis=-1)\n",
    "    customdata_normalized_RB  = np.stack((RB_normalized['nom'],RB_normalized['brasseur'],RB_normalized['normalized_rating'], RB_normalized['type'],RB_normalized['prix']), axis=-1)\n",
    "    fig_rank.update_xaxes(showgrid=False)\n",
    "    fig_rank.update_yaxes(autorange = \"reversed\")\n",
    "    fig_rank.add_trace(go.Bar(\n",
    "                                        y=RB_sorted[\"nom\"],\n",
    "                                        x=RB_sorted[\"avg\"],\n",
    "                                        orientation=\"h\",\n",
    "                                        visible=True,\n",
    "                                        width=1,\n",
    "                                        customdata=customdata_RB,\n",
    "                                        hovertemplate=\"\"\" <br><b>Brew</b>: %{customdata[0]}\n",
    "                                                        <br><b>Brewery</b>: %{customdata[1]}\n",
    "                                                        <br><b>Mean Rating</b>: %{customdata[2]:.2f}\n",
    "                                                        <br><b>Type</b>: %{customdata[3]}\n",
    "                                                        <br><b>Price</b>: %{customdata[4]}<br><extra></extra>\"\"\"),\n",
    "                            row=1,\n",
    "                            col=1,)\n",
    "    fig_rank.add_trace(go.Bar(\n",
    "                                        y=RB_normalized[\"nom\"],\n",
    "                                        x=RB_normalized[\"normalized_rating\"],\n",
    "                                        orientation=\"h\",\n",
    "                                        visible=False,\n",
    "                                        width=1,\n",
    "                                        customdata=customdata_normalized_RB,\n",
    "                                        hovertemplate=\"\"\" <br><b>Brew</b>: %{customdata[0]}\n",
    "                                                        <br><b>Brewery</b>: %{customdata[1]}\n",
    "                                                        <br><b>Rating normalized by price and serving volume</b>: %{customdata[2]:.2f}\n",
    "                                                        <br><b>Type</b>: %{customdata[3]}\n",
    "                                                        <br><b>Price</b>: %{customdata[4]}<br><extra></extra>\"\"\"),\n",
    "                            row=1,\n",
    "                            col=1,)\n",
    "    fig_rank.add_trace(go.Bar(\n",
    "                                        y=BA_sorted[\"nom\"],\n",
    "                                        x=BA_sorted[\"avg\"],\n",
    "                                        orientation=\"h\",\n",
    "                                        visible=True,\n",
    "                                        width=1,\n",
    "\n",
    "                                        customdata=customdata_BA,\n",
    "                                        hovertemplate=\"\"\" <br><b>Brew</b>: %{customdata[0]}\n",
    "                                                        <br><b>Brewery</b>: %{customdata[1]}\n",
    "                                                        <br><b>Brewery</b>: %{customdata[2]:.2f}\n",
    "                                                        <br><b>Type</b>: %{customdata[3]}\n",
    "                                                        <br><b>Price</b>: %{customdata[4]}<br><extra></extra>\"\"\"),\n",
    "                            row=1,\n",
    "                            col=2,)\n",
    "    fig_rank.add_trace(go.Bar(\n",
    "                                        y=BA_normalized[\"nom\"],\n",
    "                                        x=BA_normalized[\"normalized_rating\"],\n",
    "                                        orientation=\"h\",\n",
    "                                        visible=False,\n",
    "                                        width=1,\n",
    "                                        customdata=customdata_normalized_BA,\n",
    "                                        hovertemplate=\"\"\" <br><b>Brew</b>: %{customdata[0]}\n",
    "                                                        <br><b>Brewery</b>: %{customdata[1]}\n",
    "                                                        <br><b>Rating normalized by price and serving volume</b>: %{customdata[2]:.2f}\n",
    "                                                        <br><b>Type</b>: %{customdata[3]}\n",
    "                                                        <br><b>Price</b>: %{customdata[4]}<br><extra></extra>\"\"\"),\n",
    "                            row=1,\n",
    "                            col=2,)\n",
    "\n",
    "\n",
    "    fig_rank.update_layout( autosize=False,\n",
    "                            margin=dict(t=0, b=0, l=0, r=0),\n",
    "                            paper_bgcolor='rgba(0,0,0,0)',\n",
    "                            plot_bgcolor='rgba(0,0,0,0)', \n",
    "                            xaxis_title=\"Beer rating\",\n",
    "                            yaxis_title=\"Beer name\",\n",
    "                       )\n",
    "\n",
    "    fig_rank.update_layout(showlegend=False,\n",
    "                            annotations=[\n",
    "                                dict(text=\"Normalize by price and volume:\", x=0.88, xref=\"paper\", y=1.05, yref=\"paper\",\n",
    "                                 align=\"left\", showarrow=False)],\n",
    "                           title_text='Ranking of SAT beers according to RateBeer and BeerAdvocate',\n",
    "                           width=900,\n",
    "                           height=1000,\n",
    "                           xaxis2=dict(range=[0, 5],title=\"RateBeer rating\"),\n",
    "                           xaxis1=dict(range=[0, 5],title=\"BeerAdvocate rating\"), \n",
    "                           updatemenus=[\n",
    "                                dict(\n",
    "                                    type=\"buttons\",\n",
    "                                    direction=\"right\",\n",
    "                                    active=0,\n",
    "                                    x=1,\n",
    "                                    y=1.05,\n",
    "                                    buttons=([\n",
    "                                        dict(label=\"No\",\n",
    "                                             method=\"update\",\n",
    "                                             args=[{\"visible\": [True, False, True, False]}\n",
    "                                                  ]),\n",
    "                                        dict(label=\"Yes\",\n",
    "                                             method=\"update\",\n",
    "                                             args=[{\"visible\": [False, True, False, True]}\n",
    "                                                   ]),\n",
    "                                                ]),\n",
    "                                            )\n",
    "                                        ],\n",
    "                           font=dict(size=10))\n",
    "    fig_rank.show(config= dict(\n",
    "                displayModeBar = False))\n",
    "    fig_rank.write_html(\"Images/sat_rank_separated.html\",config = {'displayModeBar': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:50:38.743514Z",
     "start_time": "2022-12-21T19:50:38.743506Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_CSV_SIZE = 1000000\n",
    "CHUNK_SIZE = 200\n",
    "def fetch_satellite_data():\n",
    "    return 0\n",
    "COLUMNS_NAMES = [\"beer_name\",\"beer_id\",\"brewery_name\",\"brewery_id\",\"style\",\"abv\",\"date\",\"username\",\"user_id\",\"appearance\",\"aroma\",\"palate\",\"taste\",\"overall\",\"rating\",\"text\"]\n",
    "\n",
    "def fetch_csv(dataset_path, name):\n",
    "    with tarfile.open(dataset_path) as tar:\n",
    "        dataframe = pd.DataFrame()\n",
    "        for filename in tar.getnames():\n",
    "            if name in filename:   \n",
    "                with tar.extractfile(filename) as file:\n",
    "                    dataframe = pd.read_csv(file)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def fetch_reviews(dataset_path, max_csv_size = MAX_CSV_SIZE,early_stop = 0):\n",
    "    \"\"\"dumps ratings and/or reviews that are in a large text file to multiple csv files of max_csv_size length.\n",
    "        \n",
    "        Parameters\n",
    "        ----------                                  \n",
    "        dataset_path  (dataframe) :  path to the tar file containing the dataset\n",
    "        max_csv_size  (int)       : max size of the csvs created by the function\n",
    "        early_stop    (int)       : for debugging purposes. Stops function at early_stop csvs created. Default is 0 and creates as many csvs as needed\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        location of user with doubled user_ids\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    tarfile_name = re.search(\"[ \\w-]+?(?=\\.)\",dataset_path)[0]\n",
    "    with tarfile.open(dataset_path) as tar:\n",
    "        datadumps = [filename for filename in tar.getnames() if \"txt.gz\" in filename]\n",
    "        print(datadumps)\n",
    "        filename = input(\"Please choose a file from the list above to open: \")\n",
    "        with tar.extractfile(filename) as file:\n",
    "            with gzip.open(file,'rt') as f:\n",
    "                review = []\n",
    "                review_dict = {}\n",
    "                row_count = 0\n",
    "                csv_count = 0\n",
    "                for line in f:\n",
    "                    if len(line) < 2:\n",
    "                        review_dict[row_count] = review\n",
    "                        row_count += 1\n",
    "                        review = []\n",
    "                        if row_count % max_csv_size == 0:\n",
    "                            df = pd.DataFrame.from_dict(review_dict, orient=\"index\")\n",
    "                            df.columns = COLUMNS_NAMES\n",
    "                            df.to_csv(f\"DATA/{tarfile_name.replace('.tar','')}_{filename.replace('.txt.gz','')}_part_{csv_count}.csv\")\n",
    "                            del review_dict #Just to not kill my memory please disregard :)\n",
    "                            review_dict = {}\n",
    "                            print(f\"Dumping data to csv number {csv_count}...\")\n",
    "                            csv_count += 1\n",
    "                            if early_stop and csv_count == early_stop : \n",
    "                                break\n",
    "                    else:\n",
    "                        (key,value) = line.split(\": \", 1)\n",
    "                        #BeerAdvocate has one column called \"review\" that is useless and makes everything harder\n",
    "                        if key != \"review\":\n",
    "                            review.append(value.rstrip())\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(review_dict, orient=\"index\")\n",
    "    df.columns=COLUMNS_NAMES\n",
    "    df.to_csv(f\"DATA/{tarfile_name.replace('.tar','')}_{filename.replace('.txt.gz','')}_part_{csv_count}.csv\")\n",
    "    del review_dict #Just to not kill my memory please disregard :)\n",
    "    print(f\"Dumping data to csv number {csv_count}...\")\n",
    "    print(\"Success!\")\n",
    "    if early_stop :\n",
    "        return 1\n",
    "    else : \n",
    "        return df\n",
    "#    except: \n",
    "#        print(\"Euh, no file was found in this path\")\n",
    "\n",
    "def fetch_satellite_df():\n",
    "\n",
    "    url = \"https://satellite.bar/bar/\"\n",
    "    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "    beer = re.findall('var djangoBoissons = (.*?);\\s*$', soup.prettify(), re.M)\n",
    "    beerJson = beer[0]\n",
    "    satellite_dict = json.loads(beerJson)\n",
    "    satellite_df = pd.DataFrame(satellite_dict)\n",
    "\n",
    "    return satellite_df\n",
    "\n",
    "\n",
    "\n",
    "def find_favourite_beers(website,threshold=10):\n",
    "    \"\"\"\n",
    "        Calculates the beers/beer styles with most votes and with biggest ratings (given there are at least 'threshold ratings')\n",
    "        \n",
    "        Parameters\n",
    "    ----------                                  \n",
    "    website         (string)  :  name of the website/dataset considered\n",
    "    threshold       (int)     :  The minimum number of ratings a beer/style need to be considered a valid best beer/style\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    most_reviewed_beer    (dataframe)\n",
    "    favorite_beer         (dataframe)\n",
    "    most_reviewed_style   (dataframe)\n",
    "    favorite_style        (dataframe)\n",
    "        \n",
    "    \"\"\"\n",
    "    def correct_double_user_id(users,row):\n",
    "        \"\"\" Routine that corrects the location of users with multiple user_id (case of Ratebeer dataset, when user_id is doubled, one of the entried has no location)\n",
    "            To be used as an argument of pandas.DataFrame.apply()\n",
    "            Parameters\n",
    "        ----------                                  \n",
    "        users         (dataframe) :  dataframe with user information\n",
    "        row           (array)     :  row in dataframe corresponding to user with two user_ids \n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        location of user with doubled user_ids\n",
    "\n",
    "        \"\"\"\n",
    "        name = row[\"username\"].strip()\n",
    "        return users[users[\"user_name\"] == name]['location'].values[0]\n",
    "    users = fetch_csv(f\"DATA/{website}.tar\",\"users\")\n",
    "    #Unknown location for some users, some users have user_id duplicates either in the users.csv (in this case, no location is available) or in ratings.txt:\n",
    "    users[\"location\"] = users[\"location\"].fillna(\"Unknown\")\n",
    "    beer_most_drinked_by_country =pd.DataFrame([])\n",
    "    style_most_drinked_by_country =pd.DataFrame([])\n",
    "\n",
    "    if website == \"RateBeer\":\n",
    "        TOTAL_CSV = 15\n",
    "        acronym = \"RB\"\n",
    "    if website == \"BeerAdvocate\":\n",
    "        TOTAL_CSV = 17\n",
    "        acronym = \"BA\"\n",
    "    print(f\"Task : find favourite beers/styles of users. Processing {website} dataset: {TOTAL_CSV} csv files in total.\")\n",
    "    for number in range(0,TOTAL_CSV):\n",
    "        temp = pd.read_csv(f\"DATA/{website}_ratings_part_{number}_corrected_w_attenuation.csv\")\n",
    "        \n",
    "        temp= temp.merge(users[[\"user_id\",\"location\"]], how=\"left\",left_on= \"user_id\",right_on=\"user_id\")\n",
    "        temp[\"user_id\"] = temp[\"user_id\"].apply(lambda x : str(x).strip()) \n",
    "        double_id_rows = temp[temp[\"location\"].isna()]\n",
    "        if len(double_id_rows) != 0:\n",
    "            temp.loc[temp[\"location\"].isna(),\"location\"] = temp.loc[temp[\"location\"].isna()].apply(lambda row : correct_double_user_id(users,row),axis=1).apply(str)\n",
    "    \n",
    "        temp_grouped_on_beer = temp[[\"location\",\"beer_name\",\"beer_id\",\"style\",\"brewery_name\",\"user_id\",\"rating\"]].groupby(by=[\"location\",\"beer_name\",\"beer_id\",\"style\",\"brewery_name\"]).agg({\"user_id\":\"count\",\"rating\":\"sum\"}).reset_index()\n",
    "        temp_grouped_on_style = temp[[\"location\",\"style\",\"user_id\",\"rating\"]].groupby(by=[\"location\",\"style\"]).agg({\"user_id\":\"count\",\"rating\":\"sum\"}).reset_index()\n",
    "        beer_most_drinked_by_country = pd.concat([beer_most_drinked_by_country, temp_grouped_on_beer]).groupby(['location', 'beer_name',\"beer_id\",\"style\",\"brewery_name\"]).sum().reset_index()\n",
    "        style_most_drinked_by_country =  pd.concat([style_most_drinked_by_country, temp_grouped_on_style]).groupby(['location', 'style']).sum().reset_index()\n",
    "    \n",
    "    print(\"Calculating most rated and best rated beers and styles...\")\n",
    "    #We renormalize the averages that were aggregated from all parts of the dataset\n",
    "    beer_most_drinked_by_country[\"normalized_rating\"] = beer_most_drinked_by_country.apply(lambda row: row[\"rating\"]/row[\"user_id\"],axis=1)\n",
    "    style_most_drinked_by_country[\"normalized_rating\"] = style_most_drinked_by_country.apply(lambda row: row[\"rating\"]/row[\"user_id\"],axis=1)\n",
    "\n",
    "    #Standardize column names before saving work\n",
    "    BEER_COLUMN_NAMES = [\"location\",\"beer_name\",\"beer_id\",\"style\",\"brewery_name\",\"count\",\"cumulated_rating\",\"normalized_rating\"]\n",
    "    STYLE_COLUMN_NAMES = [\"location\",\"style\",\"count\",\"cumulated_rating\",\"normalized_rating\"]\n",
    "    beer_most_drinked_by_country.columns = BEER_COLUMN_NAMES\n",
    "    style_most_drinked_by_country.columns = STYLE_COLUMN_NAMES\n",
    "\n",
    "    def most_rated(df):\n",
    "        df = df.reset_index()\n",
    "        row = df.iloc[df['count'].idxmax()].copy()\n",
    "        return row\n",
    "    def best_rating(df,threshold,metric):\n",
    "        df = df.reset_index()\n",
    "        row = df.loc[[0]].copy()\n",
    "        df = df[df[\"count\"] > threshold].copy()\n",
    "        if len(df) == 0:\n",
    "            row[metric] = \"No beer with enough votes\"\n",
    "            if metric == \"beer_name\":\n",
    "                row[\"beer_id\"] = -1\n",
    "                row[\"style\"] = \"-\"\n",
    "                row[\"brewery_name\"] = \"-\"\n",
    "                row[\"normalized_rating\"] = \"0\"\n",
    "            return row\n",
    "        else : \n",
    "            row = df.loc[[df['normalized_rating'].idxmax()]].copy()\n",
    "            return row\n",
    "    most_reviewed_beer = beer_most_drinked_by_country.groupby(by=\"location\").apply(lambda df : most_rated(df)).reset_index(drop=True)\n",
    "    favorite_beer = beer_most_drinked_by_country.groupby(by=\"location\").apply(lambda df : best_rating(df,threshold,\"beer_name\")).reset_index(drop=True)\n",
    "\n",
    "    most_reviewed_style = style_most_drinked_by_country.groupby(by=\"location\").apply(lambda df : most_rated(df)).reset_index(drop=True)\n",
    "    favorite_style = style_most_drinked_by_country.groupby(by=\"location\").apply(lambda df : best_rating(df,threshold,\"style\")).reset_index(drop=True)\n",
    "    print(\"Success!\")\n",
    "    return most_reviewed_beer,favorite_beer,most_reviewed_style,favorite_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAT_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T19:50:38.746747Z",
     "start_time": "2022-12-21T19:50:38.746544Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "RB_style_dict = {\n",
    "            \"IPA\" : 'India Pale Ale (IPA)',\n",
    "            \"Blanche\" : \"Belgian Ale\",\n",
    "            \"White IPA\": \"India Pale Ale (IPA)\",\n",
    "            \"Sour\": 'Sour/Wild Ale',\n",
    "            \"Blonde\" : 'Golden Ale/Blond Ale',\n",
    "            \"New England IPA\" : \"American Pale Ale\",\n",
    "            \"Imperial Stout\" : \"Imperial Stout\",\n",
    "            \"Berliner Weisse\" : 'Berliner Weisse',\n",
    "            \"Ambrée\": 'Amber Ale',\n",
    "            \"Pale Ale\" : 'English Pale Ale'\n",
    "        }\n",
    "\n",
    "RB_countries_dict = {\n",
    "            'Royaume-Uni' : 'England',\n",
    "            \"Suisse\" : \"Switzerland\",\n",
    "            \"Norvège\": \"Norway\",\n",
    "            \"Allemagne\": 'Germany',\n",
    "            \"Pays-Bas\" : \"Netherlands\",\n",
    "            \"Pologne\" : \"Poland\",\n",
    "            \"Espagne\" : 'Spain',\n",
    "            \"France\" : \"France\"\n",
    "        }\n",
    "\n",
    "BA_style_dict = {\n",
    "            \"IPA\" : 'English India Pale Ale (IPA)',\n",
    "            \"Blanche\" : 'Belgian IPA',\n",
    "            \"White IPA\": 'American Pale Wheat Ale',\n",
    "            \"Blonde\" : 'Belgian IPA',\n",
    "            \"Lambic\" : 'Lambic - Fruit',\n",
    "            \"Sour\": 'Extra Special / Strong Bitter (ESB)',\n",
    "            \"New England IPA\" : 'American IPA',\n",
    "            \"Imperial Stout\" : 'American Double / Imperial Stout',\n",
    "            \"Berliner Weisse\" : 'Berliner Weissbier',\n",
    "            \"Ambrée\": 'American Amber / Red Ale',\n",
    "            \"Pale Ale\" :  'American Pale Ale (APA)',\n",
    "            \"Imperial IPA\" :  'American Double / Imperial IPA' \n",
    "        }\n",
    "\n",
    "BA_countries_dict = {\n",
    "        'Royaume-Uni' : 'England',\n",
    "        \"Suisse\" : \"Switzerland\",\n",
    "        \"Norvège\": \"Norway\",\n",
    "        \"Allemagne\": 'Germany',\n",
    "        \"Pays-Bas\" : \"Netherlands\",\n",
    "        \"Pologne\" : \"Poland\",\n",
    "        \"Espagne\" : 'Spain',\n",
    "        \"France\" : \"France\",\n",
    "        \"Belgique\" : \"Belgium\"}\n",
    "\n",
    "def generate_automatic_beer_matches(website,matched_dataset,corrected_beers_df):\n",
    "    \"\"\"Generates a dataframe with the items/beers of a given 'website' dataset that have more than 0.8 cosine similarity with SAT beers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    website         (string)     :  Name of the dataset. Either 'RateBeer' or 'BeerAdvocate'.\n",
    "    matched_dataset (dataframe)  :  Dataframe with all matches between SAT beers and dataset\n",
    "    corrected_beers_df (dataframe): Dataframe with bias-corrected ratings for each beer\n",
    "    Returns\n",
    "    -------\n",
    "    (dataframe) : dataframe of all SAT beers that found a reasonable match in the 'website' dataframe\n",
    "    (dataframe) : dataframe of all SAT beers that have not found a reasonable match in the 'website' dataframe\n",
    "    (dataframe) : dataframe with top 5 retrievals of SAT beers without match. Used for manual matching in following step of the pipeline\n",
    "    \"\"\"\n",
    "    if website == \"RateBeer\":\n",
    "        acronym = \"RB\"\n",
    "    if website == \"BeerAdvocate\":\n",
    "        acronym = \"BA\"\n",
    "    SAT_beers = read_data.fetch_satellite_df()\n",
    "    SAT_match_candidates = matched_dataset\n",
    "    beers = corrected_beers_df\n",
    "    beers =  beers[beers[\"nbr_ratings\"] != 0].copy()\n",
    "    mask = ((SAT_match_candidates[\"alcool\"] == SAT_match_candidates[f\"{acronym}_abv\"]) & (SAT_match_candidates[f\"{acronym}_similarity\"] > 0.8))\n",
    "    automatic_matches = SAT_match_candidates[mask][[\"nom\",f\"{acronym}_beer_name\",f\"{acronym}_avg\",f\"{acronym}_abv\",f\"{acronym}_similarity\",f\"{acronym}_brewery_name\",f\"{acronym}_style\",f\"{acronym}_beer_id\"]].drop_duplicates(subset=\"nom\", keep='first', inplace=False, ignore_index=False)\n",
    "    \n",
    "    not_matched =  SAT_match_candidates[~mask][[\"nom\",f\"{acronym}_beer_name\",f\"{acronym}_avg\",f\"{acronym}_abv\",f\"{acronym}_similarity\",f\"{acronym}_brewery_name\",f\"{acronym}_style\",f\"{acronym}_beer_id\"]].drop_duplicates(subset=\"nom\", keep='first', inplace=False, ignore_index=False)\n",
    "    not_matched =  SAT_beers[~SAT_beers[\"nom\"].isin(automatic_matches[\"nom\"].unique())]\n",
    "    top5_for_manual_matching = SAT_match_candidates[~SAT_match_candidates[\"nom\"].isin(automatic_matches[\"nom\"].unique())]\n",
    "    return automatic_matches,not_matched, top5_for_manual_matching\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features(website,matched_dataset,corrected_beers_df):\n",
    "    \"\"\" Constructs feature vectors for beers. \n",
    "        These features are used for training a model on rating estimation and/or to estimate ratings\n",
    "        . Feature vectors consists of:\n",
    "        - Alcohol content (float), \n",
    "        - dummy variables for country of origin of the brewery (int)\n",
    "        - dummy variables for the beer style\n",
    "    \n",
    "        When given the name of the dataset ('website') and the dataset subset ('matched_dataset')\n",
    "    consisting only of beers sold at SAT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    website         (string)     :  Name of the dataset. Either 'RateBeer' or 'BeerAdvocate' \n",
    "                                 \n",
    "    matched_dataset (dataframe)  : dataframe of all the beers sold on SAT that were found \n",
    "                on the dataset corresponding with 'website'\n",
    "\n",
    "    corrected_beers_df (dataframe) :  Dataframe with bias-corrected ratings for each beer\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (dataframe) : dataframe without the rating, but with abv (alcohol content) value and \n",
    "                dummy variables for all considered features. Used for estimating ratings of SAT beers\n",
    "    (dataframe) : dataframe with the rating given in the 'website' dataset. Used to train the model\n",
    "    \n",
    "    \"\"\"\n",
    "    if website == \"RateBeer\":\n",
    "        style_dict = {\n",
    "            \"IPA\" : 'India Pale Ale (IPA)',\n",
    "            \"Blanche\" : \"Belgian Ale\",\n",
    "            \"White IPA\": \"India Pale Ale (IPA)\",\n",
    "            \"Sour\": 'Sour/Wild Ale',\n",
    "            \"Blonde\" : 'Golden Ale/Blond Ale',\n",
    "            \"New England IPA\" : \"American Pale Ale\",\n",
    "            \"Imperial Stout\" : \"Imperial Stout\",\n",
    "            \"Berliner Weisse\" : 'Berliner Weisse',\n",
    "            \"Ambrée\": 'Amber Ale',\n",
    "            \"Pale Ale\" : 'English Pale Ale'\n",
    "        }\n",
    "        countries_dict = {\n",
    "            'Royaume-Uni' : 'England',\n",
    "            \"Suisse\" : \"Switzerland\",\n",
    "            \"Norvège\": \"Norway\",\n",
    "            \"Allemagne\": 'Germany',\n",
    "            \"Pays-Bas\" : \"Netherlands\",\n",
    "            \"Pologne\" : \"Poland\",\n",
    "            \"Espagne\" : 'Spain',\n",
    "            \"France\" : \"France\"\n",
    "        }\n",
    "    if website == \"BeerAdvocate\":\n",
    "        style_dict = {\n",
    "            \"IPA\" : 'English India Pale Ale (IPA)',\n",
    "            \"Blanche\" : 'Belgian IPA',\n",
    "            \"White IPA\": 'American Pale Wheat Ale',\n",
    "            \"Blonde\" : 'Belgian IPA',\n",
    "            \"Lambic\" : 'Lambic - Fruit',\n",
    "            \"Sour\": 'Extra Special / Strong Bitter (ESB)',\n",
    "            \"New England IPA\" : 'American IPA',\n",
    "            \"Imperial Stout\" : 'American Double / Imperial Stout',\n",
    "            \"Berliner Weisse\" : 'Berliner Weissbier',\n",
    "            \"Ambrée\": 'American Amber / Red Ale',\n",
    "            \"Pale Ale\" :  'American Pale Ale (APA)',\n",
    "            \"Imperial IPA\" :  'American Double / Imperial IPA' \n",
    "        }\n",
    "        countries_dict = {\n",
    "        'Royaume-Uni' : 'England',\n",
    "        \"Suisse\" : \"Switzerland\",\n",
    "        \"Norvège\": \"Norway\",\n",
    "        \"Allemagne\": 'Germany',\n",
    "        \"Pays-Bas\" : \"Netherlands\",\n",
    "        \"Pologne\" : \"Poland\",\n",
    "        \"Espagne\" : 'Spain',\n",
    "        \"France\" : \"France\",\n",
    "        \"Belgique\" : \"Belgium\"}\n",
    "    SAT_beers = read_data.fetch_satellite_df()\n",
    "\n",
    "    beers_to_predict = SAT_beers.loc[~SAT_beers[\"nom\"].isin(matched_dataset[\"nom\"])]\n",
    "    beers_to_predict[\"type\"] = beers_to_predict[\"type\"].apply(lambda x : style_dict[x])\n",
    "    beers_to_predict[\"from\"] = beers_to_predict[\"from\"].apply(lambda x : countries_dict[x])\n",
    "    beers = corrected_beers_df\n",
    "    features_for_traning = beers[[\"abv\",\"location\",\"style\",\"avg\"]]\n",
    "    features_for_traning.dropna(subset=\"avg\",axis='index',inplace=True)\n",
    "    features_for_traning.fillna(0,inplace=True)\n",
    "    SAT_features = beers_to_predict[[\"alcool\",\"from\",\"type\"]]\n",
    "    SAT_features.columns = [\"abv\",\"location\",\"style\"]\n",
    "    sat_beers_to_rate = pd.concat([features_for_traning[[\"abv\",\"location\",\"style\"]],SAT_features],axis=0)\n",
    "    sat_beers_to_rate_with_dummies =pd.get_dummies(sat_beers_to_rate,columns=[\"style\",\"location\"])\n",
    "    features = pd.get_dummies(features_for_traning,columns=[\"style\",\"location\"])\n",
    "    return sat_beers_to_rate_with_dummies.tail(len(beers_to_predict)), features,beers_to_predict, features_for_traning\n",
    "\n",
    "\n",
    "def randomforest_sat_beers_ratings(features_to_train,features_to_estimate):\n",
    "    \"\"\" Trains a RandomForestRegressor with 'features_to_train' in order to estimate ratings of beers corresponding to 'features_to_estimate'\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features_to_train    (dataframe)  :  dataframe of features used to train the dataset. Column '1' should be the labels. \n",
    "                                 \n",
    "    features_to_estimate (dataframe)  : dataframe of features used to estimate beer ratings of beers without a match.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (float) : r2 score of the regression performed\n",
    "    (np.array) : array with predicted ratings\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X = pd.concat([features_to_train.iloc[:,0],features_to_train.iloc[:,2:len(features_to_train.columns)]],axis=1)\n",
    "    y = features_to_train[\"avg\"]\n",
    "    clf = RandomForestRegressor()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3 )\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_fitted = clf.predict(X_test)\n",
    "    r2_score_result = r2_score(y_test,y_fitted)\n",
    "    predictions = clf.predict(features_to_estimate)\n",
    "    return r2_score_result, predictions  \n",
    "\n",
    "\n",
    "def save_and_display_sat_ratings(website,predictions,matching_results,beers_to_predict,training_set):\n",
    "    \"\"\"\n",
    "    saves and displays rating estimation results to a csv and display the full set of ratings of sat beers. \n",
    "    Ratings correspond to : \n",
    "    - Ratings that were automatically matched with generate_automatic_beer_matches()\n",
    "    - Ratings that were manually matched\n",
    "    - Ratings that were estimated with a regressor model with randomforest_sat_beers_ratings()\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    website          (string)    : Name of the dataset. Either 'RateBeer' or 'BeerAdvocate'. \n",
    "                                 \n",
    "    predictions      (np.array)  : Array of predicted ratings for beers without a match in the 'website' dataframe.\n",
    "\n",
    "    matching_results (dataframe) : Ratings for beers that found a match in 'website' dataframe.\n",
    "    \n",
    "    training_set     (dataframe) : dataframe of features used to train the dataset. Column '1' should be the labels. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    if website == \"BeerAdvocate\":\n",
    "        acronym = \"BA\"\n",
    "    if website == \"RateBeer\":\n",
    "        acronym = \"RB\"\n",
    "    SAT_beers = read_data.fetch_satellite_df()\n",
    "    beers_to_predict[\"predictions\"] = predictions\n",
    "    naive_average = training_set.groupby(by=[\"abv\",\"location\"]).agg({\"mean\"})\n",
    "    SAT_ratings = beers_to_predict.merge(naive_average,how=\"left\",left_on=[\"alcool\",\"from\"],right_on=[\"abv\",\"location\"])\n",
    "    SAT_beers = SAT_beers.merge(matching_results[[\"nom\",f\"{acronym}_avg\",f\"{acronym}_beer_id\"]],how=\"left\",on=\"nom\")\n",
    "    SAT_results = SAT_beers.merge(SAT_ratings[[\"nom\",\"predictions\",('avg', 'mean')]],how=\"left\",left_on=\"nom\",right_on=\"nom\")\n",
    "    #III.5. As a sanity check and to have an alternative for our model, we naively compute averages \n",
    "    #of beers that come from the same country and have the same ABV. We ignore information about beer type\n",
    "    #in order to have averages over bigger sets (otherwise, some combinations of (origin,abv,type) would have a single element)\n",
    "    SAT_results[f'{acronym}_avg'].fillna(SAT_results['predictions'],inplace=True)\n",
    "    SAT_results.rename({f'{acronym}_avg' : 'avg'},inplace=True,axis=1)\n",
    "    SAT_results.drop_duplicates(subset=\"nom\", keep='first', inplace=True, ignore_index=False)\n",
    "    SAT_results.sort_values(by=\"avg\",ascending=False,inplace=True)\n",
    "    SAT_results.to_csv(f\"data/predicted_SAT_{acronym}_sorted.csv\",index=True)\n",
    "    display(SAT_results[[\"nom\",\"type\",\"brasseur\",\"avg\"]])\n",
    "\n",
    "    return SAT_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
